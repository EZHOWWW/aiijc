{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d60d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.32.4)\n",
      "Requirement already satisfied: tqdm in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: trimesh in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.7.1)\n",
      "Requirement already satisfied: thingi10k in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy-stl in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: datasets>=4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (4.0.0)\n",
      "Requirement already satisfied: polars>=1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (1.32.0)\n",
      "Requirement already satisfied: lagrange-open>=6.29.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (6.35.0)\n",
      "Requirement already satisfied: python-utils>=3.4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from numpy-stl) (3.9.1)\n",
      "Requirement already satisfied: filelock in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.34.3)\n",
      "Requirement already satisfied: packaging in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (1.1.5)\n",
      "Requirement already satisfied: colorama>=0.4.4 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from lagrange-open>=6.29.0->thingi10k) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0->thingi10k) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm trimesh thingi10k numpy-stl numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cc90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from scipy) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import trimesh\n",
    "import thingi10k\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "import warnings\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4839166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetsManager(root_dir='data')\n",
      "CPU workers: 12, IO workers: 16\n",
      "Downloading custom dataset zips from Yandex.Disk (parallel)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5058742a3b514cae9e3504c60366744c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Yandex downloads:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45edd1cd6e6b456cb174cc5e3a424ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "yandex_train_data.zip:   0%|          | 0.00/3.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8f1715530a42319cd6e382b0588426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "yandex_test_data.zip:   0%|          | 0.00/633M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dataset done.\n"
     ]
    }
   ],
   "source": [
    "# datasets_manager_fixed.py\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import platform\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "# mesh libs\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "import thingi10k\n",
    "import scipy.io\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "\n",
    "# -----------------------\n",
    "# Worker functions (module-level for pickling)\n",
    "# -----------------------\n",
    "def _download_stream(url, filename, timeout=30, max_retries=3, chunk_size=8192):\n",
    "    \"\"\"\n",
    "    Download via requests for http/https; fallback to urllib for ftp.\n",
    "    Returns filename on success or raises.\n",
    "    \"\"\"\n",
    "    if url.startswith(\"ftp://\"):\n",
    "        # use urllib.request.urlretrieve for ftp\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            if os.path.exists(filename):\n",
    "                try: os.remove(filename)\n",
    "                except Exception: pass\n",
    "            raise RuntimeError(f\"FTP download failed: {url} -> {e}\")\n",
    "    # else HTTP(S)\n",
    "    session = requests.Session()\n",
    "    adapter = requests.adapters.HTTPAdapter(max_retries=max_retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    try:\n",
    "        with session.get(url, stream=True, timeout=timeout) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0) or 0)\n",
    "            with open(filename, \"wb\") as f, tqdm(total=total, unit=\"B\", unit_scale=True, leave=False, desc=os.path.basename(filename)) as bar:\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        if os.path.exists(filename):\n",
    "            try: os.remove(filename)\n",
    "            except Exception: pass\n",
    "        raise RuntimeError(f\"Download failed: {url} -> {e}\")\n",
    "\n",
    "def _convert_to_stl_worker(src_path, dst_path):\n",
    "    \"\"\"\n",
    "    Convert a single mesh file to STL using trimesh.\n",
    "    Returns (dst_path, None) on success or (dst_path, error_str) on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(dst_path):\n",
    "            return (dst_path, None)\n",
    "        # trimesh can sometimes infer format; try load_mesh, and if fails try load\n",
    "        mesh_obj = None\n",
    "        try:\n",
    "            mesh_obj = trimesh.load_mesh(src_path, force='mesh')\n",
    "        except Exception:\n",
    "            try:\n",
    "                mesh_obj = trimesh.load(src_path, force='mesh')\n",
    "            except Exception as e:\n",
    "                return (dst_path, f\"trimesh load failed: {e}\")\n",
    "\n",
    "        if mesh_obj is None or mesh_obj.is_empty:\n",
    "            return (dst_path, \"Empty or invalid mesh\")\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        mesh_obj.export(dst_path)\n",
    "        return (dst_path, None)\n",
    "    except Exception as e:\n",
    "        return (dst_path, str(e))\n",
    "\n",
    "def _thingi10k_np_to_stl_worker(npz_path, _id, dst_path):\n",
    "    \"\"\"\n",
    "    Convert Thingi10k stored numpy arrays to STL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(dst_path):\n",
    "            return (dst_path, None)\n",
    "        with np.load(npz_path) as data:\n",
    "            vertices = np.asarray(data[\"vertices\"], dtype=np.float64)\n",
    "            facets = np.asarray(data[\"facets\"], dtype=np.int32)\n",
    "        if vertices.shape[0] < 3 or facets.shape[0] == 0:\n",
    "            return (dst_path, \"Insufficient geometry\")\n",
    "        mesh_data = vertices[facets]\n",
    "        stl_mesh = mesh.Mesh(np.zeros(mesh_data.shape[0], dtype=mesh.Mesh.dtype))\n",
    "        stl_mesh.vectors = mesh_data\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        stl_mesh.save(dst_path)\n",
    "        return (dst_path, None)\n",
    "    except Exception as e:\n",
    "        return (dst_path, str(e))\n",
    "\n",
    "# -----------------------\n",
    "# Manager\n",
    "# -----------------------\n",
    "class DatasetsManager:\n",
    "    def __init__(self, root_dir=\"data\", max_workers_cpu=None, max_workers_io=None):\n",
    "        self.root_dir = root_dir\n",
    "        # Note: user requested final structure modelnet40, shapenet, etc.\n",
    "        self.paths = {\n",
    "            \"thingi10k\": os.path.join(root_dir, \"thingi10k\"),\n",
    "            \"modelnet\": os.path.join(root_dir, \"modelnet40\"),   # << specifically to match your request\n",
    "            \"abc\": os.path.join(root_dir, \"abc_dataset\"),\n",
    "            \"objectnet\": os.path.join(root_dir, \"objectnet3d\"),\n",
    "            \"shapenet\": os.path.join(root_dir, \"shapenet\"),\n",
    "            \"custom\": os.path.join(root_dir, \"custom_dataset\"),\n",
    "        }\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "        cpu_count = max(1, (os.cpu_count() or 2) - 1)\n",
    "        self.max_workers_cpu = max_workers_cpu or min(max(1, cpu_count), 12)\n",
    "        self.max_workers_io = max_workers_io or min(16, (os.cpu_count() or 4) * 2)\n",
    "\n",
    "        print(f\"DatasetsManager(root_dir='{self.root_dir}')\")\n",
    "        print(f\"CPU workers: {self.max_workers_cpu}, IO workers: {self.max_workers_io}\")\n",
    "\n",
    "    # ---- Process context helpers ----\n",
    "    def _get_process_context(self):\n",
    "        try:\n",
    "            if platform.system() != \"Windows\":\n",
    "                return multiprocessing.get_context(\"fork\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        return multiprocessing.get_context()\n",
    "\n",
    "    def _use_process_pool(self):\n",
    "        ctx = self._get_process_context()\n",
    "        try:\n",
    "            with ProcessPoolExecutor(max_workers=1, mp_context=ctx) as ex:\n",
    "                pass\n",
    "            return ctx\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Process pool unavailable: {e}. Will fallback to threads.\")\n",
    "            return None\n",
    "\n",
    "    # ---- parallel convert / download ----\n",
    "    def _parallel_mesh_convert(self, jobs, desc=\"Converting meshes\"):\n",
    "        results = {}\n",
    "        if not jobs:\n",
    "            return results\n",
    "\n",
    "        ctx = self._use_process_pool()\n",
    "        if ctx is not None:\n",
    "            with ProcessPoolExecutor(max_workers=self.max_workers_cpu, mp_context=ctx) as exe:\n",
    "                futures = {}\n",
    "                for src, dst, jtype in jobs:\n",
    "                    if jtype == \"thingi10k\":\n",
    "                        fut = exe.submit(_thingi10k_np_to_stl_worker, src, os.path.basename(src), dst)\n",
    "                    else:\n",
    "                        fut = exe.submit(_convert_to_stl_worker, src, dst)\n",
    "                    futures[fut] = dst\n",
    "                for fut in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "                    dst = futures[fut]\n",
    "                    try:\n",
    "                        dst_path, err = fut.result()\n",
    "                        results[dst_path] = err\n",
    "                    except Exception as e:\n",
    "                        results[dst] = str(e)\n",
    "                        print(str(e))\n",
    "            return results\n",
    "\n",
    "        # fallback to threads (less efficient for CPU-bound)\n",
    "        logging.warning(\"Falling back to ThreadPoolExecutor for conversions.\")\n",
    "        with ThreadPoolExecutor(max_workers=max(2, min(self.max_workers_cpu, 8))) as exe:\n",
    "            futures = {}\n",
    "            for src, dst, jtype in jobs:\n",
    "                if jtype == \"thingi10k\":\n",
    "                    fut = exe.submit(_thingi10k_np_to_stl_worker, src, os.path.basename(src), dst)\n",
    "                else:\n",
    "                    fut = exe.submit(_convert_to_stl_worker, src, dst)\n",
    "                futures[fut] = dst\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "                dst = futures[fut]\n",
    "                try:\n",
    "                    dst_path, err = fut.result()\n",
    "                    results[dst_path] = err\n",
    "                except Exception as e:\n",
    "                    results[dst] = str(e)\n",
    "        return results\n",
    "\n",
    "    def _parallel_downloads(self, download_tasks, desc=\"Downloading\"):\n",
    "        results = []\n",
    "        if not download_tasks:\n",
    "            return results\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers_io) as exe:\n",
    "            futures = {exe.submit(_download_stream, url, out): (url, out) for (url, out) in download_tasks}\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "                url, out = futures[fut]\n",
    "                try:\n",
    "                    res = fut.result()\n",
    "                    results.append((out, None))\n",
    "                except Exception as e:\n",
    "                    results.append((out, str(e)))\n",
    "        return results\n",
    "\n",
    "    # ---- orchestrator ----\n",
    "    def prepare_all_datasets(self):\n",
    "        order = [\n",
    "            self.prepare_thingi10k,\n",
    "            self.prepare_modelnet40,\n",
    "            self.prepare_abc_dataset,\n",
    "            self.prepare_objectnet3d,\n",
    "            self.prepare_shapenet,\n",
    "            self.prepare_custom_dataset\n",
    "        ]\n",
    "        for i, fn in enumerate(order, start=1):\n",
    "            print(f\"\\n--- [{i}/{len(order)}] {fn.__name__} ---\")\n",
    "            fn()\n",
    "        print(\"\\nAll dataset preparations finished.\")\n",
    "\n",
    "    # ---------- Thingi10k ----------\n",
    "    def prepare_thingi10k(self):\n",
    "        out_models = os.path.join(self.paths[\"thingi10k\"], \"models\")\n",
    "        os.makedirs(out_models, exist_ok=True)\n",
    "        try:\n",
    "            thingi10k.init()\n",
    "        except Exception as e:\n",
    "            print(f\"Thingi10k init failed: {e}\")\n",
    "            return\n",
    "        jobs = []\n",
    "        for entry in tqdm(thingi10k.dataset(), desc=\"Collect Thingi10k entries\"):\n",
    "            fid = entry.get(\"file_id\")\n",
    "            npz_path = entry.get(\"file_path\")\n",
    "            if not npz_path or not fid:\n",
    "                continue\n",
    "            dst = os.path.join(out_models, f\"{fid}.stl\")\n",
    "            if os.path.exists(dst):\n",
    "                continue\n",
    "            jobs.append((npz_path, dst, \"thingi10k\"))\n",
    "        if not jobs:\n",
    "            print(\"No new Thingi10k jobs.\")\n",
    "            return\n",
    "        print(f\"Converting {len(jobs)} Thingi10k entries using up to {self.max_workers_cpu} workers...\")\n",
    "        res = self._parallel_mesh_convert(jobs, desc=\"Thingi10k -> STL\")\n",
    "        failed = [p for p, e in res.items() if e]\n",
    "        if failed:\n",
    "            print(f\"Thingi10k: {len(failed)} conversions failed.\")\n",
    "        print(\"Thingi10k done.\")\n",
    "\n",
    "    # ---------- ModelNet40 ----------\n",
    "    def prepare_modelnet40(self):\n",
    "        url = \"http://modelnet.cs.princeton.edu/ModelNet40.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"ModelNet40.zip\")\n",
    "        out_root = self.paths[\"modelnet\"]\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "        # skip if already processed (detect any category/models/*.stl)\n",
    "        if os.path.exists(out_root):\n",
    "            found = False\n",
    "            for cat in os.scandir(out_root):\n",
    "                if not cat.is_dir(): continue\n",
    "                models_dir = os.path.join(cat.path, \"models\")\n",
    "                if os.path.exists(models_dir) and any(f.name.endswith(\".stl\") for f in os.scandir(models_dir)):\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                print(\"ModelNet40 seems processed; skipping.\")\n",
    "                return\n",
    "\n",
    "        # download and extract if not present\n",
    "        if not os.path.exists(out_root):\n",
    "            print(\"Downloading ModelNet40 zip...\")\n",
    "            dl = self._parallel_downloads([(url, zip_path)], desc=\"Downloading ModelNet40\")\n",
    "            if not os.path.exists(zip_path):\n",
    "                print(\"ModelNet40 zip missing after download.\")\n",
    "                return\n",
    "            print(\"Extracting ModelNet40...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                z.extractall(self.root_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        jobs = []\n",
    "        # The on-disk structure after extraction is ModelNet40/[category]/train/*.off and test/*.off\n",
    "        top_src = os.path.join(self.root_dir, \"ModelNet40\")\n",
    "        if not os.path.exists(top_src):\n",
    "            print(f\"Expected extracted folder {top_src} not found; aborting ModelNet40.\")\n",
    "            return\n",
    "\n",
    "        for category_d in tqdm(os.scandir(top_src), desc=\"Collecting ModelNet categories\"):\n",
    "            if not category_d.is_dir() or category_d.name.startswith(\"__\"):\n",
    "                continue\n",
    "            category_name = category_d.name\n",
    "            dest_models_dir = os.path.join(out_root, category_name, \"models\")\n",
    "            os.makedirs(dest_models_dir, exist_ok=True)\n",
    "\n",
    "            for split in [\"train\", \"test\"]:\n",
    "                split_path = os.path.join(category_d.path, split)\n",
    "                if not os.path.exists(split_path):\n",
    "                    continue\n",
    "                for off_file in os.scandir(split_path):\n",
    "                    if off_file.name.endswith(\".off\"):\n",
    "                        dst = os.path.join(dest_models_dir, off_file.name.replace(\".off\", \".stl\"))\n",
    "                        if os.path.exists(dst):\n",
    "                            continue\n",
    "                        jobs.append((off_file.path, dst, \"general\"))\n",
    "                # Remove split directory to save space AFTER collecting jobs\n",
    "                try:\n",
    "                    shutil.rmtree(split_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if jobs:\n",
    "            print(f\"Converting {len(jobs)} ModelNet files (into {out_root}) ...\")\n",
    "            res = self._parallel_mesh_convert(jobs, desc=\"ModelNet40 conversions\")\n",
    "            failed = sum(1 for e in res.values() if e)\n",
    "            print(f\"ModelNet40 conversions finished. Failed: {failed}\")\n",
    "        else:\n",
    "            print(\"No ModelNet conversions needed.\")\n",
    "        print(\"ModelNet40 done.\")\n",
    "\n",
    "    # ---------- ABC Dataset ----------\n",
    "    def prepare_abc_dataset(self):\n",
    "        url = \"https://archive.nyu.edu/retrieve/120666/abc_full_100k_v00.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"abc_full_100k_v00.zip\")\n",
    "        extracted_path = os.path.join(self.root_dir, \"100k\")\n",
    "        models_out = os.path.join(self.paths[\"abc\"], \"models\")\n",
    "        os.makedirs(models_out, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(models_out) and len(os.listdir(models_out)) > 1000:\n",
    "            print(\"ABC dataset seems processed; skipping.\")\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(extracted_path):\n",
    "            print(\"Downloading ABC dataset (large)...\")\n",
    "            self._parallel_downloads([(url, zip_path)], desc=\"Downloading ABC\")\n",
    "            if not os.path.exists(zip_path):\n",
    "                print(\"ABC zip not present after download.\")\n",
    "                return\n",
    "            print(\"Extracting ABC (this will take a while)...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                z.extractall(self.root_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        jobs = []\n",
    "        for split in [\"train/512\", \"test/512\"]:\n",
    "            src_dir = os.path.join(extracted_path, split)\n",
    "            if not os.path.exists(src_dir):\n",
    "                continue\n",
    "            for file in os.scandir(src_dir):\n",
    "                if file.name.endswith(\".obj\"):\n",
    "                    dst = os.path.join(models_out, file.name.replace(\".obj\", \".stl\"))\n",
    "                    if os.path.exists(dst):\n",
    "                        continue\n",
    "                    jobs.append((file.path, dst, \"general\"))\n",
    "\n",
    "        if jobs:\n",
    "            print(f\"Converting {len(jobs)} ABC models...\")\n",
    "            res = self._parallel_mesh_convert(jobs, desc=\"ABC conversions\")\n",
    "            failed = sum(1 for e in res.values() if e)\n",
    "            print(f\"ABC conversions complete. Failed: {failed}\")\n",
    "        else:\n",
    "            print(\"No ABC conversions needed.\")\n",
    "\n",
    "        if os.path.exists(extracted_path):\n",
    "            try:\n",
    "                shutil.rmtree(extracted_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not remove ABC extracted folder: {e}\")\n",
    "\n",
    "    # ---------- ObjectNet3D ----------\n",
    "    def prepare_objectnet3d(self):\n",
    "        out_root = self.paths[\"objectnet\"]\n",
    "        os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "        # If directory has contents assume already processed (keeps idempotency)\n",
    "        if any(os.scandir(out_root)):\n",
    "            print(\"ObjectNet3D directory not empty; skipping.\")\n",
    "            return\n",
    "\n",
    "        temp_dir = os.path.join(self.root_dir, \"ObjectNet3D_temp\")\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        try:\n",
    "            urls = {\n",
    "                \"annotations\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_annotations.zip\",\n",
    "                \"cads\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_cads.zip\",\n",
    "                \"images\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_images.zip\"\n",
    "            }\n",
    "            dl_tasks = []\n",
    "            for name, url in urls.items():\n",
    "                target_zip = os.path.join(temp_dir, os.path.basename(url))\n",
    "                if not os.path.exists(target_zip):\n",
    "                    dl_tasks.append((url, target_zip))\n",
    "            if dl_tasks:\n",
    "                print(\"Downloading ObjectNet3D archives (parallel; FTP supported)...\")\n",
    "                dl_res = self._parallel_downloads(dl_tasks, desc=\"ObjectNet3D downloads\")\n",
    "                for out, err in dl_res:\n",
    "                    if err:\n",
    "                        print(f\"Download failed for {out}: {err}\")\n",
    "\n",
    "            # Extract any zips found\n",
    "            for file in os.scandir(temp_dir):\n",
    "                if file.name.endswith(\".zip\") and os.path.getsize(file.path) > 0:\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(file.path, \"r\") as z:\n",
    "                            z.extractall(temp_dir)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Extraction failed {file.path}: {e}\")\n",
    "\n",
    "            # Convert CAD .off -> .stl\n",
    "            cad_off_dir = os.path.join(temp_dir, \"ObjectNet3D\", \"CAD\", \"off\")\n",
    "            jobs = []\n",
    "            if os.path.exists(cad_off_dir):\n",
    "                for cat in os.scandir(cad_off_dir):\n",
    "                    if not cat.is_dir(): continue\n",
    "                    dst_cat_models = os.path.join(out_root, cat.name, \"models\")\n",
    "                    os.makedirs(dst_cat_models, exist_ok=True)\n",
    "                    for off_file in os.scandir(cat.path):\n",
    "                        # earlier code required 6-char base name; maintain that check\n",
    "                        if off_file.name.endswith(\".off\") and len(off_file.name.split(\".\")[0]) == 2:\n",
    "                            dst = os.path.join(dst_cat_models, off_file.name.replace(\".off\", \".stl\"))\n",
    "                            if os.path.exists(dst):\n",
    "                                continue\n",
    "                            jobs.append((off_file.path, dst, \"general\"))\n",
    "\n",
    "            if jobs:\n",
    "                print(f\"Converting {len(jobs)} ObjectNet3D CAD files...\")\n",
    "                res = self._parallel_mesh_convert(jobs, desc=\"ObjectNet3D CAD -> STL\")\n",
    "                failed = sum(1 for e in res.values() if e)\n",
    "                print(f\"ObjectNet3D CAD conversions done. Failed: {failed}\")\n",
    "            else:\n",
    "                print(\"No CAD conversion jobs found for ObjectNet3D.\")\n",
    "\n",
    "            # Link/copy images using annotations\n",
    "            ann_dir = os.path.join(temp_dir, \"ObjectNet3D\", \"Annotations\")\n",
    "            img_dir = os.path.join(temp_dir, \"ObjectNet3D\", \"Images\")\n",
    "            if os.path.exists(ann_dir) and os.path.exists(img_dir):\n",
    "                for mat_file in tqdm(os.scandir(ann_dir), desc=\"ObjectNet3D annotations\"):\n",
    "                    if not (mat_file.name.endswith(\".mat\") and len(mat_file.name.split('.')[0]) == 6):\n",
    "                        continue\n",
    "                    try:\n",
    "                        mat = scipy.io.loadmat(mat_file.path)\n",
    "                        record = mat['record'][0, 0]\n",
    "                        img_filename = str(record['filename'][0])\n",
    "                        category_name = str(record['objects'][0, 0]['class'][0])\n",
    "                        src_img = os.path.join(img_dir, img_filename)\n",
    "                        if not os.path.exists(src_img):\n",
    "                            continue\n",
    "                        dest_img_dir = os.path.join(out_root, category_name, \"images\")\n",
    "                        os.makedirs(dest_img_dir, exist_ok=True)\n",
    "                        shutil.copy(src_img, os.path.join(dest_img_dir, os.path.basename(src_img)))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Annotation processing error {mat_file.name}: {e}\")\n",
    "            else:\n",
    "                print(\"ObjectNet3D annotations or images folder missing after extraction.\")\n",
    "        finally:\n",
    "            # cleanup\n",
    "            try:\n",
    "                if os.path.exists(temp_dir):\n",
    "                    pass\n",
    "                    shutil.rmtree(temp_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(\"ObjectNet3D done.\")\n",
    "\n",
    "    # ---------- ShapeNet ----------\n",
    "    def prepare_shapenet(self):\n",
    "        out_root = self.paths[\"shapenet\"]\n",
    "        os.makedirs(out_root, exist_ok=True)\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        if not hf_token:\n",
    "            print(\"HF_TOKEN not set; skipping ShapeNet.\")\n",
    "            return\n",
    "        try:\n",
    "            from huggingface_hub import login, snapshot_download\n",
    "            login(token=hf_token)\n",
    "            repo_id = \"ShapeNet/ShapeNetCore\"\n",
    "            cache_dir = snapshot_download(repo_id=repo_id, repo_type=\"dataset\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch ShapeNet from HF Hub: {e}\")\n",
    "            return\n",
    "\n",
    "        jobs = []\n",
    "        tmp_dirs_for_cleanup = []\n",
    "        # iterate over snapshot zips\n",
    "        for zip_filename in tqdm(os.listdir(cache_dir), desc=\"Enumerate ShapeNet zips\"):\n",
    "            if not zip_filename.endswith(\".zip\"):\n",
    "                continue\n",
    "            cat_id = zip_filename[:-4]\n",
    "            models_dir = os.path.join(out_root, cat_id, \"models\")\n",
    "            images_dir = os.path.join(out_root, cat_id, \"screenshots\")  # use \"screenshots\" folder specifically\n",
    "            if os.path.exists(models_dir) and len(os.listdir(models_dir)) > 0:\n",
    "                continue\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            os.makedirs(images_dir, exist_ok=True)\n",
    "            zip_filepath = os.path.join(cache_dir, zip_filename)\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_filepath, \"r\") as z:\n",
    "                    all_files = z.namelist()\n",
    "                    # find model ids\n",
    "                    model_ids = sorted(list({f.split(\"/\")[1] for f in all_files if f.count(\"/\") >= 2}))\n",
    "                    for mid in model_ids:\n",
    "                        # find model .obj in models/ folder\n",
    "                        model_norm = f\"{cat_id}/{mid}/models/model_normalized.obj\"\n",
    "                        candidate = None\n",
    "                        if model_norm in all_files:\n",
    "                            candidate = model_norm\n",
    "                        else:\n",
    "                            for f in all_files:\n",
    "                                if f.startswith(f\"{cat_id}/{mid}/models/\") and f.endswith(\".obj\"):\n",
    "                                    candidate = f\n",
    "                                    break\n",
    "                        if not candidate:\n",
    "                            continue\n",
    "                        dst = os.path.join(models_dir, f\"{mid}.stl\")\n",
    "                        if os.path.exists(dst):\n",
    "                            continue\n",
    "                        # Extract the candidate into a temp dir (so worker can read it)\n",
    "                        tmp_dir = tempfile.mkdtemp(prefix=\"shapenet_\")\n",
    "                        try:\n",
    "                            # ensure extraction path keeps internal folder structure so src_path exists\n",
    "                            z.extract(candidate, path=tmp_dir)\n",
    "                            src_path = os.path.join(tmp_dir, candidate)\n",
    "                            if not os.path.exists(src_path):\n",
    "                                # some zip entries may not preserve paths - attempt to find the extracted file\n",
    "                                # fallback: search tmp_dir for *.obj\n",
    "                                found_obj = None\n",
    "                                for root, _, files in os.walk(tmp_dir):\n",
    "                                    for f in files:\n",
    "                                        if f.endswith(\".obj\"):\n",
    "                                            found_obj = os.path.join(root, f)\n",
    "                                            break\n",
    "                                    if found_obj:\n",
    "                                        break\n",
    "                                if found_obj:\n",
    "                                    src_path = found_obj\n",
    "                                else:\n",
    "                                    raise RuntimeError(f\"Could not locate extracted OBJ for {candidate}\")\n",
    "                            jobs.append((src_path, dst, \"general\"))\n",
    "                            tmp_dirs_for_cleanup.append(tmp_dir)\n",
    "                        except Exception as ex:\n",
    "                            shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "                            print(f\"Error extracting {candidate} from {zip_filename}: {ex}\")\n",
    "\n",
    "                    # Extract screenshots only from the screenshots/ folder of each model\n",
    "                    for f in all_files:\n",
    "                        # pattern: {cat_id}/{mid}/screenshots/{something}.png\n",
    "                        if f.count(\"/\") >= 3 and f.startswith(f\"{cat_id}/\") and \"/screenshots/\" in f and (f.endswith(\".png\") or f.endswith(\".jpg\") or f.endswith(\".jpeg\")):\n",
    "                            parts = f.split(\"/\")\n",
    "                            # parts[1] is model id\n",
    "                            mid = parts[1]\n",
    "                            basename = os.path.basename(f)\n",
    "                            outname = f\"{mid}_{basename}\"\n",
    "                            outpath = os.path.join(images_dir, outname)\n",
    "                            if not os.path.exists(outpath):\n",
    "                                try:\n",
    "                                    with z.open(f) as src, open(outpath, \"wb\") as dst:\n",
    "                                        shutil.copyfileobj(src, dst)\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing ShapeNet zip {zip_filename}: {e}\")\n",
    "\n",
    "        if jobs:\n",
    "            print(f\"Converting {len(jobs)} ShapeNet models...\")\n",
    "            res = self._parallel_mesh_convert(jobs, desc=\"ShapeNet conversions\")\n",
    "            failed = sum(1 for e in res.values() if e)\n",
    "            print(f\"ShapeNet conversions done. Failed: {failed}\")\n",
    "\n",
    "        for tmp_dir in set(tmp_dirs_for_cleanup):\n",
    "            try:\n",
    "                shutil.rmtree(tmp_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(\"ShapeNet done.\")\n",
    "\n",
    "    # ---------- Custom dataset (Yandex.Disk) ----------\n",
    "    def prepare_custom_dataset(self):\n",
    "        out_root = self.paths[\"custom\"]\n",
    "        os.makedirs(out_root, exist_ok=True)\n",
    "        if any(os.scandir(out_root)):\n",
    "            print(\"Custom dataset folder not empty; skipping.\")\n",
    "            return\n",
    "\n",
    "        files_to_download = {\n",
    "            \"train_data\": \"https://disk.yandex.ru/d/RRXJu9ZtEmSXzQ\",\n",
    "            \"test_data\": \"https://disk.yandex.ru/d/TmbB7BsGzg1dQQ\",\n",
    "        }\n",
    "        tasks = []\n",
    "        for key, public_url in files_to_download.items():\n",
    "            api = \"https://cloud-api.yandex.net/v1/disk/public/resources/download?\"\n",
    "            api_url = api + urlencode(dict(public_key=public_url))\n",
    "            try:\n",
    "                r = requests.get(api_url, timeout=10)\n",
    "                r.raise_for_status()\n",
    "                href = r.json().get(\"href\")\n",
    "                if href:\n",
    "                    out_zip = os.path.join(self.root_dir, f\"yandex_{key}.zip\")\n",
    "                    tasks.append((href, out_zip))\n",
    "                else:\n",
    "                    print(f\"Could not resolve Yandex link {public_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Yandex API error for {public_url}: {e}\")\n",
    "\n",
    "        if tasks:\n",
    "            print(\"Downloading custom dataset zips from Yandex.Disk (parallel)...\")\n",
    "            dl_res = self._parallel_downloads(tasks, desc=\"Yandex downloads\")\n",
    "            for (zipfile_path, err) in dl_res:\n",
    "                if err:\n",
    "                    print(f\"Download failed for {zipfile_path}: {err}\")\n",
    "                    continue\n",
    "                try:\n",
    "                    with zipfile.ZipFile(zipfile_path, \"r\") as z:\n",
    "                        z.extractall(out_root)\n",
    "                    os.remove(zipfile_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {zipfile_path}: {e}\")\n",
    "        print(\"Custom dataset done.\")\n",
    "\n",
    "# -----------------------\n",
    "# If executed as a script\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Recommended: run this script from a shell for maximum stability.\n",
    "    manager = DatasetsManager(root_dir=\"data\")\n",
    "    manager.prepare_custom_dataset()\n",
    "    # manager.prepare_all_datasets()\n",
    "    # manager.prepare_modelnet40()\n",
    "    # manager.prepare_shapenet()\n",
    "    # manager.prepare_objectnet3d()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiijcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
