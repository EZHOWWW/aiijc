{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d60d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.32.4)\n",
      "Requirement already satisfied: tqdm in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: trimesh in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.7.1)\n",
      "Requirement already satisfied: thingi10k in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy-stl in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: datasets>=4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (4.0.0)\n",
      "Requirement already satisfied: polars>=1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (1.32.0)\n",
      "Requirement already satisfied: lagrange-open>=6.29.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (6.35.0)\n",
      "Requirement already satisfied: python-utils>=3.4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from numpy-stl) (3.9.1)\n",
      "Requirement already satisfied: filelock in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.34.3)\n",
      "Requirement already satisfied: packaging in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (1.1.5)\n",
      "Requirement already satisfied: colorama>=0.4.4 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from lagrange-open>=6.29.0->thingi10k) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0->thingi10k) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm trimesh thingi10k numpy-stl numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88cc90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from scipy) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import trimesh\n",
    "import thingi10k\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "import warnings\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetsManager initialized. All data will be stored in 'my_3d_datasets'\n",
      "\n",
      "[5/6] Processing ShapeNetCore from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e55bbc461d34cbc856e4259d3177df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 58 files:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeNet data available in cache: /home/dima/.cache/huggingface/hub/datasets--ShapeNet--ShapeNetCore/snapshots/0efb24cbe6828a85771a28335c5f7b5626514d9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ShapeNet Categories: 100%|██████████| 58/58 [05:33<00:00,  5.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeNetCore preparation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import thingi10k\n",
    "from stl import mesh\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from urllib.parse import urlencode\n",
    "from huggingface_hub import login, snapshot_download\n",
    "\n",
    "class DatasetsManager:\n",
    "    \"\"\"\n",
    "    A manager class to download, process, and structure 3D datasets for ML tasks.\n",
    "    Follows a consistent structure: data/[dataset]/[category]/[type]/[file]\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir=\"data\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.thingi10k_dir = os.path.join(root_dir, \"thingi10k\")\n",
    "        self.modelnet_dir = os.path.join(root_dir, \"ModelNet40\")\n",
    "        self.abc_dir = os.path.join(root_dir, \"abc_dataset\")\n",
    "        self.objectnet_dir = os.path.join(root_dir, \"objectnet3d\")\n",
    "        self.shapenet_dir = os.path.join(root_dir, \"shapenet\")\n",
    "        self.custom_data_dir = os.path.join(root_dir, \"custom_dataset\")\n",
    "        \n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "        print(f\"DatasetsManager initialized. All data will be stored in '{self.root_dir}'\")\n",
    "\n",
    "    def prepare_all_datasets(self):\n",
    "        print(\"\\n--- Starting all dataset preparation processes ---\")\n",
    "        self.prepare_thingi10k()\n",
    "        self.prepare_modelnet40()\n",
    "        self.prepare_abc_dataset()\n",
    "        self.prepare_objectnet3d()\n",
    "        self.prepare_shapenet()\n",
    "        self.prepare_custom_dataset()\n",
    "        print(\"\\n--- All dataset preparation processes are complete! ---\")\n",
    "    \n",
    "    # ... [Остальные методы (prepare_thingi10k, prepare_modelnet40, и т.д.) остаются без изменений] ...\n",
    "    def prepare_thingi10k(self):\n",
    "        print(\"\\n[1/6] Processing Thingi10k...\")\n",
    "        models_out_dir = os.path.join(self.thingi10k_dir, \"models\")\n",
    "        os.makedirs(models_out_dir, exist_ok=True)\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"thingi10k\")\n",
    "        try:\n",
    "            thingi10k.init()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not initialize Thingi10k dataset. Maybe servers are down? Error: {e}\")\n",
    "            return\n",
    "\n",
    "        for entry in tqdm(thingi10k.dataset(), desc=\"Converting Thingi10k\"):\n",
    "            file_id = entry[\"file_id\"]\n",
    "            output_filepath = os.path.join(models_out_dir, f\"{file_id}.stl\")\n",
    "            if os.path.exists(output_filepath):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with np.load(entry[\"file_path\"]) as data:\n",
    "                    vertices = np.asarray(data[\"vertices\"], dtype=np.float64)\n",
    "                    facets = np.asarray(data[\"facets\"], dtype=np.int32)\n",
    "                \n",
    "                if vertices.shape[0] < 3 or facets.shape[0] == 0: continue\n",
    "\n",
    "                mesh_data = vertices[facets]\n",
    "                stl_mesh = mesh.Mesh(np.zeros(mesh_data.shape[0], dtype=mesh.Mesh.dtype))\n",
    "                stl_mesh.vectors = mesh_data\n",
    "                stl_mesh.save(output_filepath)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping Thingi10k file_id {file_id}. Error: {e}\")\n",
    "        print(\"Thingi10k preparation complete.\")\n",
    "\n",
    "    def prepare_modelnet40(self):\n",
    "        print(\"\\n[2/6] Processing ModelNet40...\")\n",
    "        url = \"http://modelnet.cs.princeton.edu/ModelNet40.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"ModelNet40.zip\")\n",
    "        \n",
    "        if os.path.exists(self.modelnet_dir) and any(os.scandir(self.modelnet_dir)):\n",
    "            is_processed = True\n",
    "            for item in os.scandir(self.modelnet_dir):\n",
    "                if item.is_dir():\n",
    "                    # Check if 'train' or 'test' folders are gone\n",
    "                    if 'train' in os.listdir(item.path) or 'test' in os.listdir(item.path):\n",
    "                        is_processed = False\n",
    "                        break\n",
    "            if is_processed:\n",
    "                print(\"ModelNet40 appears to be processed already. Skipping.\")\n",
    "                return\n",
    "\n",
    "        if not os.path.exists(os.path.join(self.root_dir, \"ModelNet40\")):\n",
    "             self._download_file(url, zip_path)\n",
    "             print(f\"Extracting {zip_path}...\")\n",
    "             with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                 zip_ref.extractall(self.root_dir)\n",
    "             os.remove(zip_path)\n",
    "\n",
    "        for category_d in tqdm(os.scandir(self.modelnet_dir), desc=\"Processing ModelNet40 categories\"):\n",
    "            if not category_d.is_dir() or category_d.name.startswith('__'):\n",
    "                continue\n",
    "            \n",
    "            for subfolder in [\"train\", \"test\"]:\n",
    "                subfolder_path = os.path.join(category_d.path, subfolder)\n",
    "                if not os.path.exists(subfolder_path):\n",
    "                    continue\n",
    "                \n",
    "                for off_file in os.scandir(subfolder_path):\n",
    "                    if off_file.name.endswith(\".off\"):\n",
    "                        stl_filename = off_file.name.replace(\".off\", \".stl\")\n",
    "                        stl_filepath = os.path.join(category_d.path, stl_filename)\n",
    "                        if os.path.exists(stl_filepath):\n",
    "                            continue\n",
    "                        try:\n",
    "                            mesh = trimesh.load_mesh(off_file.path)\n",
    "                            mesh.export(stl_filepath)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to process {off_file.path}: {e}\")\n",
    "                shutil.rmtree(subfolder_path)\n",
    "        print(\"ModelNet40 preparation complete.\")\n",
    "    \n",
    "    def prepare_abc_dataset(self):\n",
    "        \"\"\"[3/6] Prepares ABC Dataset: downloads, extracts, filters for '512' .obj, and converts to .stl.\"\"\"\n",
    "        print(\"\\n[3/6] Processing ABC Dataset...\")\n",
    "        url = \"https://archive.nyu.edu/retrieve/120666/abc_full_100k_v00.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"abc_full_100k_v00.zip\")\n",
    "        \n",
    "        # Correct path to the extracted folder, as seen in your screenshot.\n",
    "        extracted_path = os.path.join(self.root_dir, \"100k\") \n",
    "        \n",
    "        models_out_dir = os.path.join(self.abc_dir, \"models\")\n",
    "        os.makedirs(models_out_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(models_out_dir) and len(os.listdir(models_out_dir)) > 1000:\n",
    "             print(\"ABC Dataset appears to be processed already. Skipping.\")\n",
    "             return\n",
    "\n",
    "        if not os.path.exists(extracted_path):\n",
    "            self._download_file(url, zip_path)\n",
    "            print(f\"Extracting {zip_path} (this may take a while)...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.root_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        # Paths to the '512' folders are built from the correct base path '100k'.\n",
    "        dirs_to_process = [\n",
    "            os.path.join(extracted_path, \"train\", \"512\"),\n",
    "            os.path.join(extracted_path, \"test\", \"512\")\n",
    "        ]\n",
    "\n",
    "        for source_dir in tqdm(dirs_to_process, desc=\"Processing ABC splits\"):\n",
    "            if not os.path.exists(source_dir):\n",
    "                print(f\"Warning: Source directory not found: {source_dir}\")\n",
    "                continue\n",
    "            \n",
    "            for obj_file in tqdm(os.scandir(source_dir), desc=f\"Converting from {os.path.basename(source_dir)}\", leave=False):\n",
    "                if not obj_file.name.endswith(\".obj\"):\n",
    "                    continue\n",
    "                \n",
    "                stl_filename = obj_file.name.replace(\".obj\", \".stl\")\n",
    "                stl_filepath = os.path.join(models_out_dir, stl_filename)\n",
    "                \n",
    "                if os.path.exists(stl_filepath):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    mesh = trimesh.load_mesh(obj_file.path)\n",
    "                    mesh.export(stl_filepath)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {obj_file.path}: {e}\")\n",
    "\n",
    "        # Cleanup will now target the correct folder.\n",
    "        print(f\"Cleaning up original extracted folder: {extracted_path}\")\n",
    "        shutil.rmtree(extracted_path)\n",
    "        print(\"ABC Dataset preparation complete.\")\n",
    "        \n",
    "    def prepare_objectnet3d(self):\n",
    "        \"\"\"[4/6] Prepares ObjectNet3D: downloads, links images to models via .mat files, and converts to .stl.\"\"\"\n",
    "        print(\"\\n[4/6] Processing ObjectNet3D...\")\n",
    "        \n",
    "        os.makedirs(self.objectnet_dir, exist_ok=True)\n",
    "        if len(os.listdir(self.objectnet_dir)) > 0:\n",
    "            print(\"ObjectNet3D appears to be processed already. Skipping.\")\n",
    "            return\n",
    "\n",
    "        temp_dir = os.path.join(self.root_dir, \"ObjectNet3D_temp_processing\")\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            urls = {\n",
    "                \"annotations\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_annotations.zip\",\n",
    "                \"cads\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_cads.zip\",\n",
    "                \"images\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_images.zip\"\n",
    "            }\n",
    "            unpacked_base_path = os.path.join(temp_dir, \"ObjectNet3D\")\n",
    "\n",
    "            for name, url in urls.items():\n",
    "                if name == \"annotations\" and os.path.exists(os.path.join(unpacked_base_path, \"Annotations\")):\n",
    "                     print(f\"{name} data already exists. Skipping download and extraction.\")\n",
    "                     continue\n",
    "                \n",
    "                zip_path = os.path.join(temp_dir, os.path.basename(url))\n",
    "                self._download_file(url, zip_path)\n",
    "                print(f\"Extracting {name}...\")\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(temp_dir)\n",
    "                os.remove(zip_path)\n",
    "\n",
    "            print(\"Processing CAD models (.off -> .stl)...\")\n",
    "            source_cad_dir = os.path.join(unpacked_base_path, \"CAD\", \"off\")\n",
    "            if os.path.exists(source_cad_dir):\n",
    "                for category_dir in tqdm(os.scandir(source_cad_dir), desc=\"Converting CAD categories\"):\n",
    "                    if not category_dir.is_dir(): continue\n",
    "                    \n",
    "                    category_name = category_dir.name\n",
    "                    dest_model_dir = os.path.join(self.objectnet_dir, category_name, \"models\")\n",
    "                    os.makedirs(dest_model_dir, exist_ok=True)\n",
    "                    \n",
    "                    for off_file in os.scandir(category_dir.path):\n",
    "                        if not (off_file.name.endswith(\".off\") and len(off_file.name.split('.')[0]) == 6): continue\n",
    "                        \n",
    "                        stl_filename = off_file.name.replace(\".off\", \".stl\")\n",
    "                        stl_filepath = os.path.join(dest_model_dir, stl_filename)\n",
    "                        \n",
    "                        try:\n",
    "                            mesh = trimesh.load_mesh(off_file.path)\n",
    "                            mesh.export(stl_filepath)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to process CAD {off_file.path}: {e}\")\n",
    "\n",
    "            print(\"Linking images to categories using .mat annotations...\")\n",
    "            source_ann_dir = os.path.join(unpacked_base_path, \"Annotations\")\n",
    "            source_img_dir = os.path.join(unpacked_base_path, \"Images\")\n",
    "            \n",
    "            if os.path.exists(source_ann_dir) and os.path.exists(source_img_dir):\n",
    "                for mat_file in tqdm(os.scandir(source_ann_dir), desc=\"Processing annotations\"):\n",
    "                    if not (mat_file.name.endswith(\".mat\") and len(mat_file.name.split('.')[0]) == 6): continue\n",
    "                    \n",
    "                    try:\n",
    "                        mat = scipy.io.loadmat(mat_file.path)\n",
    "                        record = mat['record'][0, 0]\n",
    "                        img_filename = str(record['filename'][0])\n",
    "                        \n",
    "                        category_name = str(record['objects'][0, 0]['class'][0])\n",
    "                        \n",
    "                        source_img_path = os.path.join(source_img_dir, img_filename)\n",
    "                        if not os.path.exists(source_img_path): continue\n",
    "                            \n",
    "                        dest_img_dir = os.path.join(self.objectnet_dir, category_name, \"images\")\n",
    "                        os.makedirs(dest_img_dir, exist_ok=True)\n",
    "                        dest_img_path = os.path.join(dest_img_dir, os.path.basename(source_img_path))\n",
    "\n",
    "                        shutil.copy(source_img_path, dest_img_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not process annotation {mat_file.name}. Error: {e}\")\n",
    "\n",
    "        finally:\n",
    "            print(f\"Cleaning up temporary directory: {temp_dir}\")\n",
    "            if os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir)\n",
    "        print(\"ObjectNet3D preparation complete.\")\n",
    "        \n",
    "    # --- FINALLY CORRECTED METHOD FOR SHAPENET ---\n",
    "    def prepare_shapenet(self):\n",
    "        \"\"\"[5/6] Prepares ShapeNetCore: downloads from HF, filters, and structures models with their screenshots.\"\"\"\n",
    "        print(\"\\n[5/6] Processing ShapeNetCore from Hugging Face...\")\n",
    "        os.makedirs(self.shapenet_dir, exist_ok=True)\n",
    "        \n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        if not hf_token:\n",
    "            print(\"Error: Hugging Face token not found. Please set the HF_TOKEN environment variable.\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            login(token=hf_token)\n",
    "            print(\"Hugging Face login successful.\")\n",
    "            repo_id = \"ShapeNet/ShapeNetCore\"\n",
    "            hf_cache_dir = snapshot_download(repo_id=repo_id, repo_type=\"dataset\")\n",
    "            print(f\"ShapeNet data available in cache: {hf_cache_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download from Hugging Face Hub: {e}\")\n",
    "            return\n",
    "            \n",
    "        temp_obj_path = os.path.join(self.root_dir, \"shapenet_temp_model.obj\")\n",
    "\n",
    "        for zip_filename in tqdm(os.listdir(hf_cache_dir), desc=\"Processing ShapeNet Categories\"):\n",
    "            if not zip_filename.endswith('.zip'):\n",
    "                continue\n",
    "            \n",
    "            category_id = zip_filename.replace('.zip', '')\n",
    "            category_path = os.path.join(self.shapenet_dir, category_id)\n",
    "            \n",
    "            # --- FIX START ---\n",
    "            # This is the new, robust check.\n",
    "            # We check if the final destination 'models' folder exists AND is not empty.\n",
    "            dest_models_dir = os.path.join(category_path, \"models\")\n",
    "            if os.path.exists(dest_models_dir) and len(os.listdir(dest_models_dir)) > 0:\n",
    "                # print(f\"Skipping already processed category: {category_id}\")\n",
    "                continue\n",
    "            # --- FIX END ---\n",
    "            \n",
    "            dest_images_dir = os.path.join(category_path, \"images\")\n",
    "            os.makedirs(dest_models_dir, exist_ok=True)\n",
    "            os.makedirs(dest_images_dir, exist_ok=True)\n",
    "            \n",
    "            zip_filepath = os.path.join(hf_cache_dir, zip_filename)\n",
    "            \n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "                    all_files = zip_ref.namelist()\n",
    "                    \n",
    "                    model_ids = sorted(list(set(\n",
    "                        [f.split('/')[1] for f in all_files if f.count('/') >= 2]\n",
    "                    )))\n",
    "                    print(model_ids)\n",
    "\n",
    "\n",
    "                    for model_id in tqdm(model_ids, desc=f\"Models in {category_id}\", leave=False):\n",
    "                        internal_prefix = f\"{category_id}/{model_id}\"\n",
    "                        screenshot_path_prefix = f\"{internal_prefix}/screenshots/\"\n",
    "\n",
    "                        if not any(f.startswith(screenshot_path_prefix) for f in all_files):\n",
    "                            continue\n",
    "                        \n",
    "                        model_path_prefix = f\"{internal_prefix}/models/\"\n",
    "                        model_file_path_norm = f\"{model_path_prefix}model_normalized.obj\"\n",
    "                        \n",
    "                        model_file_path = model_file_path_norm if model_file_path_norm in all_files else next((f for f in all_files if f.startswith(model_path_prefix) and f.endswith(\".obj\")), None)\n",
    "                        \n",
    "                        if not model_file_path:\n",
    "                            continue\n",
    "\n",
    "                        stl_filepath = os.path.join(dest_models_dir, f\"{model_id}.stl\")\n",
    "                        if os.path.exists(stl_filepath):\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            with zip_ref.open(model_file_path) as source_file, open(temp_obj_path, \"wb\") as target_file:\n",
    "                                shutil.copyfileobj(source_file, target_file)\n",
    "                            \n",
    "                            mesh = trimesh.load_mesh(temp_obj_path)\n",
    "                            mesh.export(stl_filepath)\n",
    "                        finally:\n",
    "                            if os.path.exists(temp_obj_path):\n",
    "                                os.remove(temp_obj_path)\n",
    "\n",
    "                        screenshot_files = [f for f in all_files if f.startswith(screenshot_path_prefix) and (f.endswith('.png') or f.endswith('.jpg'))]\n",
    "                        for i, screenshot_file_path in enumerate(screenshot_files):\n",
    "                            ext = \".png\" if screenshot_file_path.endswith(\".png\") else \".jpg\"\n",
    "                            img_dest_path = os.path.join(dest_images_dir, f\"{model_id}_{i}{ext}\")\n",
    "                            \n",
    "                            with zip_ref.open(screenshot_file_path) as source, open(img_dest_path, \"wb\") as target:\n",
    "                                shutil.copyfileobj(source, target)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {zip_filename}: {e}\")\n",
    "        \n",
    "        print(\"ShapeNetCore preparation complete.\")\n",
    "\n",
    "    def prepare_custom_dataset(self):\n",
    "        \"\"\"[6/6] Downloads and structures the custom dataset from Yandex.Disk.\"\"\"\n",
    "        print(\"\\n[6/6] Processing Custom Dataset from Yandex.Disk...\")\n",
    "        os.makedirs(self.custom_data_dir, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(self.custom_data_dir) and len(os.listdir(self.custom_data_dir)) > 0:\n",
    "            print(\"Custom dataset appears to be prepared. Skipping.\")\n",
    "            return\n",
    "\n",
    "        files_to_download = {\n",
    "            \"train_data\": \"https://disk.yandex.ru/d/RRXJu9ZtEmSXzQ\",\n",
    "            \"test_data\": \"https://disk.yandex.ru/d/TmbB7BsGzg1dQQ\",\n",
    "        }\n",
    "        for key, val in files_to_download.items():\n",
    "            self.download_and_unzip_yandex_disk(val, self.custom_data_dir)\n",
    "\n",
    "        print(\"Custom dataset preparation complete.\")\n",
    "        \n",
    "    def _download_file(self, url, filename):\n",
    "        print(f\"Downloading {url} to {os.path.basename(filename)}...\")\n",
    "        try:\n",
    "            if url.startswith('ftp://'):\n",
    "                import urllib.request\n",
    "                urllib.request.urlretrieve(url, filename)\n",
    "                print(f\"FTP download of {os.path.basename(filename)} complete.\")\n",
    "            else:\n",
    "                with requests.get(url, stream=True) as r:\n",
    "                    r.raise_for_status()\n",
    "                    total_size = int(r.headers.get('content-length', 0))\n",
    "                    with open(filename, 'wb') as f, tqdm(\n",
    "                        total=total_size, unit='iB', unit_scale=True, desc=os.path.basename(filename)\n",
    "                    ) as bar:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            size = f.write(chunk)\n",
    "                            bar.update(size)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading file {url}: {e}\")\n",
    "            if os.path.exists(filename): os.remove(filename)\n",
    "            raise\n",
    "\n",
    "    def download_and_unzip_yandex_disk(self, public_url, extract_to_folder):\n",
    "        print(f\"Starting to process Yandex.Disk link: {public_url}\")\n",
    "        base_api_url = \"https://cloud-api.yandex.net/v1/disk/public/resources/download?\"\n",
    "        api_url = base_api_url + urlencode(dict(public_key=public_url))\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            response.raise_for_status()\n",
    "            download_url = response.json().get(\"href\")\n",
    "            if not download_url:\n",
    "                print(f\"Error: Could not get a direct download link for {public_url}\"); return\n",
    "\n",
    "            zip_filename = os.path.join(self.root_dir, \"temp_yandex_download.zip\")\n",
    "            self._download_file(download_url, zip_filename)\n",
    "\n",
    "            with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(extract_to_folder)\n",
    "            \n",
    "            os.remove(zip_filename)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"A network or API error occurred: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    manager = DatasetsManager(root_dir=\"my_3d_datasets\")\n",
    "    manager.prepare_shapenet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiijcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
