{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d60d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.32.4)\n",
      "Requirement already satisfied: tqdm in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: trimesh in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.7.1)\n",
      "Requirement already satisfied: thingi10k in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy-stl in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: datasets>=4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (4.0.0)\n",
      "Requirement already satisfied: polars>=1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (1.32.0)\n",
      "Requirement already satisfied: lagrange-open>=6.29.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (6.35.0)\n",
      "Requirement already satisfied: python-utils>=3.4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from numpy-stl) (3.9.1)\n",
      "Requirement already satisfied: filelock in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.34.3)\n",
      "Requirement already satisfied: packaging in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (1.1.5)\n",
      "Requirement already satisfied: colorama>=0.4.4 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from lagrange-open>=6.29.0->thingi10k) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0->thingi10k) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm trimesh thingi10k numpy-stl numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import trimesh\n",
    "import thingi10k\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a284018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetsManager initialized. All data will be stored in 'my_3d_datasets'\n",
      "\n",
      "[3/4] Processing ABC Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ABC splits: 100%|██████████| 2/2 [03:26<00:00, 103.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up original extracted folder: my_3d_datasets/100k\n",
      "ABC Dataset preparation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import thingi10k\n",
    "from stl import mesh\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "class DatasetsManager:\n",
    "    \"\"\"\n",
    "    A manager class to download, process, and structure 3D datasets for ML tasks.\n",
    "    Follows a consistent structure: data/[dataset]/[category]/[type]/[file]\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir=\"data\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.thingi10k_dir = os.path.join(root_dir, \"thingi10k\")\n",
    "        self.modelnet_dir = os.path.join(root_dir, \"ModelNet40\")\n",
    "        self.custom_data_dir = os.path.join(root_dir, \"custom_dataset\")\n",
    "        self.abc_dir = os.path.join(root_dir, \"abc_dataset\")\n",
    "        \n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "        print(f\"DatasetsManager initialized. All data will be stored in '{self.root_dir}'\")\n",
    "\n",
    "    def prepare_all_datasets(self):\n",
    "        print(\"\\n--- Starting all dataset preparation processes ---\")\n",
    "        self.prepare_thingi10k()\n",
    "        self.prepare_modelnet40()\n",
    "        self.prepare_abc_dataset()\n",
    "        self.prepare_custom_dataset()\n",
    "        print(\"\\n--- All dataset preparation processes are complete! ---\")\n",
    "\n",
    "    def prepare_thingi10k(self):\n",
    "        print(\"\\n[1/4] Processing Thingi10k...\")\n",
    "        models_out_dir = os.path.join(self.thingi10k_dir, \"models\")\n",
    "        os.makedirs(models_out_dir, exist_ok=True)\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"thingi10k\")\n",
    "        try:\n",
    "            thingi10k.init()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not initialize Thingi10k dataset. Maybe servers are down? Error: {e}\")\n",
    "            return\n",
    "\n",
    "        for entry in tqdm(thingi10k.dataset(), desc=\"Converting Thingi10k\"):\n",
    "            file_id = entry[\"file_id\"]\n",
    "            output_filepath = os.path.join(models_out_dir, f\"{file_id}.stl\")\n",
    "            if os.path.exists(output_filepath):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                with np.load(entry[\"file_path\"]) as data:\n",
    "                    vertices = np.asarray(data[\"vertices\"], dtype=np.float64)\n",
    "                    facets = np.asarray(data[\"facets\"], dtype=np.int32)\n",
    "                \n",
    "                if vertices.shape[0] < 3 or facets.shape[0] == 0: continue\n",
    "\n",
    "                mesh_data = vertices[facets]\n",
    "                stl_mesh = mesh.Mesh(np.zeros(mesh_data.shape[0], dtype=mesh.Mesh.dtype))\n",
    "                stl_mesh.vectors = mesh_data\n",
    "                stl_mesh.save(output_filepath)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping Thingi10k file_id {file_id}. Error: {e}\")\n",
    "        print(\"Thingi10k preparation complete.\")\n",
    "\n",
    "    def prepare_modelnet40(self):\n",
    "        print(\"\\n[2/4] Processing ModelNet40...\")\n",
    "        url = \"http://modelnet.cs.princeton.edu/ModelNet40.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"ModelNet40.zip\")\n",
    "        \n",
    "        if os.path.exists(self.modelnet_dir) and any(os.scandir(self.modelnet_dir)):\n",
    "            is_processed = True\n",
    "            for item in os.scandir(self.modelnet_dir):\n",
    "                if item.is_dir():\n",
    "                    # Check if 'train' or 'test' folders are gone\n",
    "                    if 'train' in os.listdir(item.path) or 'test' in os.listdir(item.path):\n",
    "                        is_processed = False\n",
    "                        break\n",
    "            if is_processed:\n",
    "                print(\"ModelNet40 appears to be processed already. Skipping.\")\n",
    "                return\n",
    "\n",
    "        if not os.path.exists(os.path.join(self.root_dir, \"ModelNet40\")):\n",
    "             self._download_file(url, zip_path)\n",
    "             print(f\"Extracting {zip_path}...\")\n",
    "             with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                 zip_ref.extractall(self.root_dir)\n",
    "             os.remove(zip_path)\n",
    "\n",
    "        for category_d in tqdm(os.scandir(self.modelnet_dir), desc=\"Processing ModelNet40 categories\"):\n",
    "            if not category_d.is_dir() or category_d.name.startswith('__'):\n",
    "                continue\n",
    "            \n",
    "            for subfolder in [\"train\", \"test\"]:\n",
    "                subfolder_path = os.path.join(category_d.path, subfolder)\n",
    "                if not os.path.exists(subfolder_path):\n",
    "                    continue\n",
    "                \n",
    "                for off_file in os.scandir(subfolder_path):\n",
    "                    if off_file.name.endswith(\".off\"):\n",
    "                        stl_filename = off_file.name.replace(\".off\", \".stl\")\n",
    "                        stl_filepath = os.path.join(category_d.path, stl_filename)\n",
    "                        if os.path.exists(stl_filepath):\n",
    "                            continue\n",
    "                        try:\n",
    "                            mesh = trimesh.load_mesh(off_file.path)\n",
    "                            mesh.export(stl_filepath)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed to process {off_file.path}: {e}\")\n",
    "                shutil.rmtree(subfolder_path)\n",
    "        print(\"ModelNet40 preparation complete.\")\n",
    "    \n",
    "    # --- ИСПРАВЛЕННЫЙ МЕТОД ---\n",
    "    def prepare_abc_dataset(self):\n",
    "        \"\"\"[3/4] Prepares ABC Dataset: downloads, extracts, filters for '512' .obj, and converts to .stl.\"\"\"\n",
    "        print(\"\\n[3/4] Processing ABC Dataset...\")\n",
    "        url = \"https://archive.nyu.edu/retrieve/120666/abc_full_100k_v00.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"abc_full_100k_v00.zip\")\n",
    "        \n",
    "        # Correct path to the extracted folder, as seen in your screenshot.\n",
    "        extracted_path = os.path.join(self.root_dir, \"100k\") \n",
    "        \n",
    "        models_out_dir = os.path.join(self.abc_dir, \"models\")\n",
    "        os.makedirs(models_out_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(models_out_dir) and len(os.listdir(models_out_dir)) > 1000:\n",
    "             print(\"ABC Dataset appears to be processed already. Skipping.\")\n",
    "             return\n",
    "\n",
    "        if not os.path.exists(extracted_path):\n",
    "            self._download_file(url, zip_path)\n",
    "            print(f\"Extracting {zip_path} (this may take a while)...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.root_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        # Paths to the '512' folders are built from the correct base path '100k'.\n",
    "        dirs_to_process = [\n",
    "            os.path.join(extracted_path, \"train\", \"512\"),\n",
    "            os.path.join(extracted_path, \"test\", \"512\")\n",
    "        ]\n",
    "\n",
    "        for source_dir in tqdm(dirs_to_process, desc=\"Processing ABC splits\"):\n",
    "            if not os.path.exists(source_dir):\n",
    "                print(f\"Warning: Source directory not found: {source_dir}\")\n",
    "                continue\n",
    "            \n",
    "            for obj_file in tqdm(os.scandir(source_dir), desc=f\"Converting from {os.path.basename(source_dir)}\", leave=False):\n",
    "                if not obj_file.name.endswith(\".obj\"):\n",
    "                    continue\n",
    "                \n",
    "                stl_filename = obj_file.name.replace(\".obj\", \".stl\")\n",
    "                stl_filepath = os.path.join(models_out_dir, stl_filename)\n",
    "                \n",
    "                if os.path.exists(stl_filepath):\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    mesh = trimesh.load_mesh(obj_file.path)\n",
    "                    mesh.export(stl_filepath)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {obj_file.path}: {e}\")\n",
    "\n",
    "        # Cleanup will now target the correct folder.\n",
    "        print(f\"Cleaning up original extracted folder: {extracted_path}\")\n",
    "        shutil.rmtree(extracted_path)\n",
    "        print(\"ABC Dataset preparation complete.\")\n",
    "        \n",
    "    def prepare_custom_dataset(self):\n",
    "        \"\"\"[4/4] Downloads and structures the custom dataset from Yandex.Disk.\"\"\"\n",
    "        print(\"\\n[4/4] Processing Custom Dataset from Yandex.Disk...\")\n",
    "        os.makedirs(self.custom_data_dir, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(self.custom_data_dir) and len(os.listdir(self.custom_data_dir)) > 0:\n",
    "            print(\"Custom dataset appears to be prepared. Skipping.\")\n",
    "            return\n",
    "\n",
    "        files_to_download = {\n",
    "            \"train_data\": \"https://disk.yandex.ru/d/RRXJu9ZtEmSXzQ\",\n",
    "            \"test_data\": \"https://disk.yandex.ru/d/TmbB7BsGzg1dQQ\",\n",
    "        }\n",
    "        for key, val in files_to_download.items():\n",
    "            self.download_and_unzip_yandex_disk(val, self.custom_data_dir)\n",
    "\n",
    "        print(\"Custom dataset preparation complete.\")\n",
    "\n",
    "    def _download_file(self, url, filename):\n",
    "        print(f\"Downloading {url} to {filename}...\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                with open(filename, 'wb') as f, tqdm(\n",
    "                    total=total_size, unit='iB', unit_scale=True, desc=os.path.basename(filename)\n",
    "                ) as bar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        size = f.write(chunk)\n",
    "                        bar.update(size)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading file: {e}\")\n",
    "            if os.path.exists(filename): os.remove(filename)\n",
    "            raise\n",
    "\n",
    "    def download_and_unzip_yandex_disk(self, public_url, extract_to_folder):\n",
    "        print(f\"Starting to process Yandex.Disk link: {public_url}\")\n",
    "        base_api_url = \"https://cloud-api.yandex.net/v1/disk/public/resources/download?\"\n",
    "        api_url = base_api_url + urlencode(dict(public_key=public_url))\n",
    "        try:\n",
    "            response = requests.get(api_url)\n",
    "            response.raise_for_status()\n",
    "            download_url = response.json().get(\"href\")\n",
    "            if not download_url:\n",
    "                print(f\"Error: Could not get a direct download link for {public_url}\"); return\n",
    "\n",
    "            zip_filename = os.path.join(self.root_dir, \"temp_yandex_download.zip\")\n",
    "            self._download_file(download_url, zip_filename)\n",
    "\n",
    "            with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(extract_to_folder)\n",
    "            \n",
    "            os.remove(zip_filename)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"A network or API error occurred: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# Example usage remains the same\n",
    "if __name__ == \"__main__\":\n",
    "    manager = DatasetsManager(root_dir=\"my_3d_datasets\")\n",
    "    # manager.prepare_all_datasets()\n",
    "    manager.prepare_abc_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ab114",
   "metadata": {},
   "source": [
    "2) ObjectNet3D Скачивается по этим ссылкам:\n",
    "модели: wget ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_cads.zip\n",
    "images: wget ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_images.zip  (большой архив)\n",
    "wget ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_annotations.zip (Для того что бы сопастовлять фото и cad)\n",
    "Этот архив устрен гораздо сложнее.\n",
    "\n",
    "архив с моделями выглядит так: [ObjectNet3D/CAD/off/[категория]/[номер, напрмер 01].obj В папке категории находятся еще  .obj файлы, но мне надо скачать только те который называются как :[номер].obj (например 03.obj). Там есть файлы например: 03_lalala_bebebe.obj, такие файлы обробатывать не нужно\n",
    "архив с моделями выглядит так: [ObjectNet3D/CAD/off/[категория]/[номер, напрмер 01].obj\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiijcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
