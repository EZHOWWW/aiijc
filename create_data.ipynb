{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d60d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.32.4)\n",
      "Requirement already satisfied: tqdm in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: trimesh in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (4.7.1)\n",
      "Requirement already satisfied: thingi10k in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy-stl in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: datasets>=4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (4.0.0)\n",
      "Requirement already satisfied: polars>=1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (1.32.0)\n",
      "Requirement already satisfied: lagrange-open>=6.29.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from thingi10k) (6.35.0)\n",
      "Requirement already satisfied: python-utils>=3.4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from numpy-stl) (3.9.1)\n",
      "Requirement already satisfied: filelock in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (0.34.3)\n",
      "Requirement already satisfied: packaging in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from datasets>=4.0->thingi10k) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=4.0->thingi10k) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets>=4.0->thingi10k) (1.1.5)\n",
      "Requirement already satisfied: colorama>=0.4.4 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from lagrange-open>=6.29.0->thingi10k) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from pandas->datasets>=4.0->thingi10k) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0->thingi10k) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm trimesh thingi10k numpy-stl numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /home/dima/miniconda3/envs/aiijcenv/lib/python3.13/site-packages (from scipy) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import trimesh\n",
    "import thingi10k\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "import warnings\n",
    "\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4839166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetsManager(root_dir='data')\n",
      "CPU workers: 12, IO workers: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "[WARNING] Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fd0aee833e4593bd2fc7e05f43eb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 58 files:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6f3b309ad64e738f0708735b2ffbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Enumerate ShapeNet zips:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0bc161b214b52a1d11cc7b0a9f1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02691156:   0%|          | 0/4045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68315e9b8ecf461eb03a04bc2dc8fa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02747177:   0%|          | 0/343 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88e2ce68ceb49d0b9dc0174a543bfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02773838:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8d7e04b633420ab1a689ef5fd83507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02801938:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543bf12c109b41c6a7a5889ed72525cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02808440:   0%|          | 0/856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7c7d67c6ca4e4fa55308102d0d8726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02818832:   0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781b916dee58442a8f34f79bbfb3c89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02828884:   0%|          | 0/1813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bf2d9feead47ecb0b7984ae87b44c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02843684:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc03e520081340ff9a8ada5c9dc0ead0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02871439:   0%|          | 0/452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012b0d7f30e542d2b02fb882368dd876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02876657:   0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b85515e89d450b87fb8cbb4eb47cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02880940:   0%|          | 0/186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffceb3848164150bd758d8593035cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02924116:   0%|          | 0/939 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680b0860188843bdbe37a06f024676d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02933112:   0%|          | 0/1571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa56afe251f43aebd33dabc9fc2d39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02942699:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74aab02ec1a4d609859fc7dcd318cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02946921:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f903443581f047eeb67b97b764e5b038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02954340:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4249e844a5e14e4c8444e4f06c75292d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02958343:   0%|          | 0/3533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e236b252740548ec87977ef71456a296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 02992529:   0%|          | 0/831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018dcce7b850434a86a9b9cb603da51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03001627:   0%|          | 0/6778 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1825a32d57f44ccd9de53316e8355f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03046257:   0%|          | 0/651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453fc96041264a0394d0b3e36b949bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03085013:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88767a87114f445a81dd273794828a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03207941:   0%|          | 0/93 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5417f6db3a60425e807749a812d3c201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03211117:   0%|          | 0/1093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ea639b607d4104b625822fdb65a6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03261776:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b1bc3ed06648b9b6b9ec6b402234a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03325088:   0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a15e5b5f6af4d5f835aa281d700bd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03337140:   0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0af09c0e19647f4ac1d46fc91bd48b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03467517:   0%|          | 0/797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a44a9b9c6a4130a1af056a38243ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03513137:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bf90a11909482b92351b89ea41e9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03593526:   0%|          | 0/596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556bf7ab8e414ac9ba921c7a58697544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03624134:   0%|          | 0/424 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7f4971d4f4439abbe6e8ac66d4fd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03636649:   0%|          | 0/2318 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d59e94a3a041a5b7960b252a7b1c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03642806:   0%|          | 0/460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20bc1d041334d1db3b95a7ba2c7c08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03691459:   0%|          | 0/1597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2886cf0e7f40c5bdc8fa2ec5709b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03710193:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807742c6a3854eb8a726e70272cfdeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03759954:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01b5ab2fac244fba2c3d60032e41223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03761084:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1bd56207294c8c8824597a456c6988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03790512:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08ba35a3c9e49b98a1a0f660f800d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03797390:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d09410cdcd43df93328a61c462e5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03928116:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f340f25ee24a0ba60a509c3909a921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03938244:   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7c77b82e004f96ac9dafe412ebadf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03948459:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea6b6b10cfd4e1283f731d88109d99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 03991062:   0%|          | 0/602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ce1ea6ab794008ab5bd69cf33731c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 04004475:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dd9886a6684ae890844a590799736c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 04074963:   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5654adfeb1c4972a4498ef3b7bafdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models in 04090263:   0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datasets_manager_fixed.py\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "import logging\n",
    "import time\n",
    "import platform\n",
    "import urllib.request\n",
    "from urllib.parse import urlencode\n",
    "from concurrent.futures import (\n",
    "    ThreadPoolExecutor,\n",
    "    ProcessPoolExecutor,\n",
    "    as_completed,\n",
    "    wait,\n",
    "    FIRST_COMPLETED,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "# mesh libs\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "import thingi10k\n",
    "import scipy.io\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Worker functions (module-level for pickling)\n",
    "# -----------------------\n",
    "def _download_stream(url, filename, timeout=30, max_retries=3, chunk_size=8192):\n",
    "    \"\"\"\n",
    "    Download via requests for http/https; fallback to urllib for ftp.\n",
    "    Returns filename on success or raises.\n",
    "    \"\"\"\n",
    "    if url.startswith(\"ftp://\"):\n",
    "        # use urllib.request.urlretrieve for ftp\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            if os.path.exists(filename):\n",
    "                try:\n",
    "                    os.remove(filename)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            raise RuntimeError(f\"FTP download failed: {url} -> {e}\")\n",
    "    # else HTTP(S)\n",
    "    session = requests.Session()\n",
    "    adapter = requests.adapters.HTTPAdapter(max_retries=max_retries)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    try:\n",
    "        with session.get(url, stream=True, timeout=timeout) as r:\n",
    "            r.raise_for_status()\n",
    "            total = int(r.headers.get(\"content-length\", 0) or 0)\n",
    "            with (\n",
    "                open(filename, \"wb\") as f,\n",
    "                tqdm(\n",
    "                    total=total,\n",
    "                    unit=\"B\",\n",
    "                    unit_scale=True,\n",
    "                    leave=False,\n",
    "                    desc=os.path.basename(filename),\n",
    "                ) as bar,\n",
    "            ):\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        if os.path.exists(filename):\n",
    "            try:\n",
    "                os.remove(filename)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise RuntimeError(f\"Download failed: {url} -> {e}\")\n",
    "\n",
    "\n",
    "def _convert_to_stl_worker(src_path, dst_path):\n",
    "    \"\"\"\n",
    "    Convert a single mesh file to STL using trimesh.\n",
    "    Returns (dst_path, None) on success or (dst_path, error_str) on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(dst_path):\n",
    "            return (dst_path, None)\n",
    "        # trimesh can sometimes infer format; try load_mesh, and if fails try load\n",
    "        mesh_obj = None\n",
    "        try:\n",
    "            mesh_obj = trimesh.load_mesh(src_path, force=\"mesh\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                mesh_obj = trimesh.load(src_path, force=\"mesh\")\n",
    "            except Exception as e:\n",
    "                return (dst_path, f\"trimesh load failed: {e}\")\n",
    "\n",
    "        if mesh_obj is None or mesh_obj.is_empty:\n",
    "            return (dst_path, \"Empty or invalid mesh\")\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        mesh_obj.export(dst_path)\n",
    "        return (dst_path, None)\n",
    "    except Exception as e:\n",
    "        return (dst_path, str(e))\n",
    "\n",
    "\n",
    "def _thingi10k_np_to_stl_worker(npz_path, _id, dst_path):\n",
    "    \"\"\"\n",
    "    Convert Thingi10k stored numpy arrays to STL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(dst_path):\n",
    "            return (dst_path, None)\n",
    "        with np.load(npz_path) as data:\n",
    "            vertices = np.asarray(data[\"vertices\"], dtype=np.float64)\n",
    "            facets = np.asarray(data[\"facets\"], dtype=np.int32)\n",
    "        if vertices.shape[0] < 3 or facets.shape[0] == 0:\n",
    "            return (dst_path, \"Insufficient geometry\")\n",
    "        mesh_data = vertices[facets]\n",
    "        stl_mesh = mesh.Mesh(np.zeros(mesh_data.shape[0], dtype=mesh.Mesh.dtype))\n",
    "        stl_mesh.vectors = mesh_data\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        stl_mesh.save(dst_path)\n",
    "        return (dst_path, None)\n",
    "    except Exception as e:\n",
    "        return (dst_path, str(e))\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Manager\n",
    "# -----------------------\n",
    "class DatasetsManager:\n",
    "    def __init__(self, root_dir=\"data\", max_workers_cpu=None, max_workers_io=None):\n",
    "        self.root_dir = root_dir\n",
    "        # Note: user requested final structure modelnet40, shapenet, etc.\n",
    "        self.paths = {\n",
    "            \"thingi10k\": os.path.join(root_dir, \"thingi10k\"),\n",
    "            \"modelnet\": os.path.join(\n",
    "                root_dir, \"modelnet40\"\n",
    "            ),  # << specifically to match your request\n",
    "            \"abc\": os.path.join(root_dir, \"abc_dataset\"),\n",
    "            \"objectnet\": os.path.join(root_dir, \"objectnet3d\"),\n",
    "            \"shapenet\": os.path.join(root_dir, \"shapenet\"),\n",
    "            \"custom\": os.path.join(root_dir, \"custom_dataset\"),\n",
    "        }\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "        cpu_count = max(1, (os.cpu_count() or 2) - 1)\n",
    "        self.max_workers_cpu = max_workers_cpu or min(max(1, cpu_count), 12)\n",
    "        self.max_workers_io = max_workers_io or min(16, (os.cpu_count() or 4) * 2)\n",
    "\n",
    "        print(f\"DatasetsManager(root_dir='{self.root_dir}')\")\n",
    "        print(f\"CPU workers: {self.max_workers_cpu}, IO workers: {self.max_workers_io}\")\n",
    "\n",
    "    # ---- Process context helpers ----\n",
    "    def _get_process_context(self):\n",
    "        try:\n",
    "            if platform.system() != \"Windows\":\n",
    "                return multiprocessing.get_context(\"fork\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        return multiprocessing.get_context()\n",
    "\n",
    "    def _use_process_pool(self):\n",
    "        ctx = self._get_process_context()\n",
    "        try:\n",
    "            with ProcessPoolExecutor(max_workers=1, mp_context=ctx) as ex:\n",
    "                pass\n",
    "            return ctx\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Process pool unavailable: {e}. Will fallback to threads.\")\n",
    "            return None\n",
    "\n",
    "    # ---- parallel convert / download ----\n",
    "    def _parallel_mesh_convert(self, jobs, desc=\"Converting meshes\"):\n",
    "        results = {}\n",
    "        if not jobs:\n",
    "            return results\n",
    "\n",
    "        ctx = self._use_process_pool()\n",
    "        if ctx is not None:\n",
    "            with ProcessPoolExecutor(\n",
    "                max_workers=self.max_workers_cpu, mp_context=ctx\n",
    "            ) as exe:\n",
    "                futures = {}\n",
    "                for src, dst, jtype in jobs:\n",
    "                    if jtype == \"thingi10k\":\n",
    "                        fut = exe.submit(\n",
    "                            _thingi10k_np_to_stl_worker, src, os.path.basename(src), dst\n",
    "                        )\n",
    "                    else:\n",
    "                        fut = exe.submit(_convert_to_stl_worker, src, dst)\n",
    "                    futures[fut] = dst\n",
    "                for fut in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "                    dst = futures[fut]\n",
    "                    try:\n",
    "                        dst_path, err = fut.result()\n",
    "                        results[dst_path] = err\n",
    "                    except Exception as e:\n",
    "                        results[dst] = str(e)\n",
    "                        print(str(e))\n",
    "            return results\n",
    "\n",
    "        # fallback to threads (less efficient for CPU-bound)\n",
    "        logging.warning(\"Falling back to ThreadPoolExecutor for conversions.\")\n",
    "        with ThreadPoolExecutor(\n",
    "            max_workers=max(2, min(self.max_workers_cpu, 8))\n",
    "        ) as exe:\n",
    "            futures = {}\n",
    "            for src, dst, jtype in jobs:\n",
    "                if jtype == \"thingi10k\":\n",
    "                    fut = exe.submit(\n",
    "                        _thingi10k_np_to_stl_worker, src, os.path.basename(src), dst\n",
    "                    )\n",
    "                else:\n",
    "                    fut = exe.submit(_convert_to_stl_worker, src, dst)\n",
    "                futures[fut] = dst\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "                dst = futures[fut]\n",
    "                try:\n",
    "                    dst_path, err = fut.result()\n",
    "                    results[dst_path] = err\n",
    "                except Exception as e:\n",
    "                    results[dst] = str(e)\n",
    "        return results\n",
    "\n",
    "    def _parallel_downloads(self, download_tasks, desc=\"Downloading\"):\n",
    "        results = []\n",
    "        if not download_tasks:\n",
    "            return results\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers_io) as exe:\n",
    "            futures = {\n",
    "                exe.submit(_download_stream, url, out): (url, out)\n",
    "                for (url, out) in download_tasks\n",
    "            }\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "                url, out = futures[fut]\n",
    "                try:\n",
    "                    res = fut.result()\n",
    "                    results.append((out, None))\n",
    "                except Exception as e:\n",
    "                    results.append((out, str(e)))\n",
    "        return results\n",
    "\n",
    "    # ---- orchestrator ----\n",
    "    def prepare_all_datasets(self):\n",
    "        order = [\n",
    "            self.prepare_thingi10k,\n",
    "            self.prepare_modelnet40,\n",
    "            self.prepare_abc_dataset,\n",
    "            self.prepare_objectnet3d,\n",
    "            self.prepare_shapenet,\n",
    "            self.prepare_custom_dataset,\n",
    "        ]\n",
    "        for i, fn in enumerate(order, start=1):\n",
    "            print(f\"\\n--- [{i}/{len(order)}] {fn.__name__} ---\")\n",
    "            fn()\n",
    "        print(\"\\nAll dataset preparations finished.\")\n",
    "\n",
    "    # ---------- Thingi10k ----------\n",
    "    def prepare_thingi10k(self):\n",
    "        out_models = os.path.join(self.paths[\"thingi10k\"], \"models\")\n",
    "        os.makedirs(out_models, exist_ok=True)\n",
    "        try:\n",
    "            thingi10k.init()\n",
    "        except Exception as e:\n",
    "            print(f\"Thingi10k init failed: {e}\")\n",
    "            return\n",
    "        jobs = []\n",
    "        for entry in tqdm(thingi10k.dataset(), desc=\"Collect Thingi10k entries\"):\n",
    "            fid = entry.get(\"file_id\")\n",
    "            npz_path = entry.get(\"file_path\")\n",
    "            if not npz_path or not fid:\n",
    "                continue\n",
    "            dst = os.path.join(out_models, f\"{fid}.stl\")\n",
    "            if os.path.exists(dst):\n",
    "                continue\n",
    "            jobs.append((npz_path, dst, \"thingi10k\"))\n",
    "        if not jobs:\n",
    "            print(\"No new Thingi10k jobs.\")\n",
    "            return\n",
    "        print(\n",
    "            f\"Converting {len(jobs)} Thingi10k entries using up to {self.max_workers_cpu} workers...\"\n",
    "        )\n",
    "        res = self._parallel_mesh_convert(jobs, desc=\"Thingi10k -> STL\")\n",
    "        failed = [p for p, e in res.items() if e]\n",
    "        if failed:\n",
    "            print(f\"Thingi10k: {len(failed)} conversions failed.\")\n",
    "        print(\"Thingi10k done.\")\n",
    "\n",
    "    # ---------- ObjectNet3D ----------\n",
    "    def prepare_objectnet3d(self):\n",
    "        out_root = self.paths[\"objectnet\"]\n",
    "        os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "        # If directory has contents assume already processed (keeps idempotency)\n",
    "        if any(os.scandir(out_root)):\n",
    "            print(\"ObjectNet3D directory not empty; skipping.\")\n",
    "            return\n",
    "\n",
    "        temp_dir = os.path.join(self.root_dir, \"ObjectNet3D_temp\")\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        try:\n",
    "            urls = {\n",
    "                \"annotations\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_annotations.zip\",\n",
    "                \"cads\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_cads.zip\",\n",
    "                \"images\": \"ftp://cs.stanford.edu/cs/cvgl/ObjectNet3D/ObjectNet3D_images.zip\",\n",
    "            }\n",
    "            dl_tasks = []\n",
    "            for name, url in urls.items():\n",
    "                target_zip = os.path.join(temp_dir, os.path.basename(url))\n",
    "                if not os.path.exists(target_zip):\n",
    "                    dl_tasks.append((url, target_zip))\n",
    "            if dl_tasks:\n",
    "                print(\"Downloading ObjectNet3D archives (parallel; FTP supported)...\")\n",
    "                dl_res = self._parallel_downloads(\n",
    "                    dl_tasks, desc=\"ObjectNet3D downloads\"\n",
    "                )\n",
    "                for out, err in dl_res:\n",
    "                    if err:\n",
    "                        print(f\"Download failed for {out}: {err}\")\n",
    "\n",
    "            # Extract any zips found\n",
    "            for file in os.scandir(temp_dir):\n",
    "                if file.name.endswith(\".zip\") and os.path.getsize(file.path) > 0:\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(file.path, \"r\") as z:\n",
    "                            z.extractall(temp_dir)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Extraction failed {file.path}: {e}\")\n",
    "\n",
    "            # Convert CAD .off -> .stl\n",
    "            cad_off_dir = os.path.join(temp_dir, \"ObjectNet3D\", \"CAD\", \"off\")\n",
    "            jobs = []\n",
    "            if os.path.exists(cad_off_dir):\n",
    "                for cat in os.scandir(cad_off_dir):\n",
    "                    if not cat.is_dir():\n",
    "                        continue\n",
    "                    dst_cat_models = os.path.join(out_root, cat.name, \"models\")\n",
    "                    os.makedirs(dst_cat_models, exist_ok=True)\n",
    "                    for off_file in os.scandir(cat.path):\n",
    "                        # earlier code required 6-char base name; maintain that check\n",
    "                        if (\n",
    "                            off_file.name.endswith(\".off\")\n",
    "                            and len(off_file.name.split(\".\")[0]) == 2\n",
    "                        ):\n",
    "                            dst = os.path.join(\n",
    "                                dst_cat_models, off_file.name.replace(\".off\", \".stl\")\n",
    "                            )\n",
    "                            if os.path.exists(dst):\n",
    "                                continue\n",
    "                            jobs.append((off_file.path, dst, \"general\"))\n",
    "\n",
    "            if jobs:\n",
    "                print(f\"Converting {len(jobs)} ObjectNet3D CAD files...\")\n",
    "                res = self._parallel_mesh_convert(jobs, desc=\"ObjectNet3D CAD -> STL\")\n",
    "                failed = sum(1 for e in res.values() if e)\n",
    "                print(f\"ObjectNet3D CAD conversions done. Failed: {failed}\")\n",
    "            else:\n",
    "                print(\"No CAD conversion jobs found for ObjectNet3D.\")\n",
    "\n",
    "            # Link/copy images using annotations\n",
    "            ann_dir = os.path.join(temp_dir, \"ObjectNet3D\", \"Annotations\")\n",
    "            img_dir = os.path.join(temp_dir, \"ObjectNet3D\", \"Images\")\n",
    "            if os.path.exists(ann_dir) and os.path.exists(img_dir):\n",
    "                for mat_file in tqdm(\n",
    "                    os.scandir(ann_dir), desc=\"ObjectNet3D annotations\"\n",
    "                ):\n",
    "                    if not (\n",
    "                        mat_file.name.endswith(\".mat\")\n",
    "                        and len(mat_file.name.split(\".\")[0]) == 6\n",
    "                    ):\n",
    "                        continue\n",
    "                    try:\n",
    "                        mat = scipy.io.loadmat(mat_file.path)\n",
    "                        record = mat[\"record\"][0, 0]\n",
    "                        img_filename = str(record[\"filename\"][0])\n",
    "                        category_name = str(record[\"objects\"][0, 0][\"class\"][0])\n",
    "                        src_img = os.path.join(img_dir, img_filename)\n",
    "                        if not os.path.exists(src_img):\n",
    "                            continue\n",
    "                        dest_img_dir = os.path.join(out_root, category_name, \"images\")\n",
    "                        os.makedirs(dest_img_dir, exist_ok=True)\n",
    "                        shutil.copy(\n",
    "                            src_img,\n",
    "                            os.path.join(dest_img_dir, os.path.basename(src_img)),\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Annotation processing error {mat_file.name}: {e}\")\n",
    "            else:\n",
    "                print(\n",
    "                    \"ObjectNet3D annotations or images folder missing after extraction.\"\n",
    "                )\n",
    "        finally:\n",
    "            # cleanup\n",
    "            try:\n",
    "                if os.path.exists(temp_dir):\n",
    "                    pass\n",
    "                    shutil.rmtree(temp_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(\"ObjectNet3D done.\")\n",
    "\n",
    "    # ---------- ModelNet40 ----------\n",
    "    def prepare_modelnet40(self):\n",
    "        url = \"http://modelnet.cs.princeton.edu/ModelNet40.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"ModelNet40.zip\")\n",
    "        out_root = self.paths[\"modelnet\"]\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "        # skip if already processed (detect any category/models/*.stl)\n",
    "        if os.path.exists(out_root):\n",
    "            found = False\n",
    "            for cat in os.scandir(out_root):\n",
    "                if not cat.is_dir():\n",
    "                    continue\n",
    "                models_dir = os.path.join(cat.path, \"models\")\n",
    "                if os.path.exists(models_dir) and any(\n",
    "                    f.name.endswith(\".stl\") for f in os.scandir(models_dir)\n",
    "                ):\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                print(\"ModelNet40 seems processed; skipping.\")\n",
    "                return\n",
    "\n",
    "        # # download and extract if not present\n",
    "        if not os.path.exists(out_root):\n",
    "            print(\"Downloading ModelNet40 zip...\")\n",
    "            dl = self._parallel_downloads(\n",
    "                [(url, zip_path)], desc=\"Downloading ModelNet40\"\n",
    "            )\n",
    "            if not os.path.exists(zip_path):\n",
    "                print(\"ModelNet40 zip missing after download.\")\n",
    "                return\n",
    "            print(\"Extracting ModelNet40...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                z.extractall(self.root_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        jobs = []\n",
    "        # The on-disk structure after extraction is ModelNet40/[category]/train/*.off and test/*.off\n",
    "        top_src = os.path.join(self.root_dir, \"ModelNet40\")\n",
    "        if not os.path.exists(top_src):\n",
    "            print(\n",
    "                f\"Expected extracted folder {top_src} not found; aborting ModelNet40.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        for category_d in tqdm(\n",
    "            os.scandir(top_src), desc=\"Collecting ModelNet categories\"\n",
    "        ):\n",
    "            if not category_d.is_dir() or category_d.name.startswith(\"__\"):\n",
    "                continue\n",
    "            category_name = category_d.name\n",
    "            dest_models_dir = os.path.join(out_root, category_name, \"models\")\n",
    "            os.makedirs(dest_models_dir, exist_ok=True)\n",
    "\n",
    "            for split in [\"train\", \"test\"]:\n",
    "                split_path = os.path.join(category_d.path, split)\n",
    "                if not os.path.exists(split_path):\n",
    "                    continue\n",
    "                for off_file in os.scandir(split_path):\n",
    "                    if off_file.name.endswith(\".off\"):\n",
    "                        dst = os.path.join(\n",
    "                            dest_models_dir, off_file.name.replace(\".off\", \".stl\")\n",
    "                        )\n",
    "                        if os.path.exists(dst):\n",
    "                            continue\n",
    "                        jobs.append((off_file.path, dst, \"general\"))\n",
    "                # Remove split directory to save space AFTER collecting jobs\n",
    "                try:\n",
    "                    shutil.rmtree(split_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if jobs:\n",
    "            print(f\"Converting {len(jobs)} ModelNet files (into {out_root}) ...\")\n",
    "            # split into batches to avoid huge queue spikes\n",
    "            batch_size = max(500, self.max_workers_cpu * 50)\n",
    "            failed_total = 0\n",
    "            for i in range(0, len(jobs), batch_size):\n",
    "                batch = jobs[i : i + batch_size]\n",
    "                res = self._parallel_mesh_convert(\n",
    "                    batch, desc=f\"ModelNet40 conversions batch {i // batch_size + 1}\"\n",
    "                )\n",
    "                failed = sum(1 for e in res.values() if e)\n",
    "                failed_total += failed\n",
    "                logging.info(f\"Batch {i // batch_size + 1}: {failed} failed\")\n",
    "            print(f\"ModelNet40 conversions finished. Failed: {failed_total}\")\n",
    "        else:\n",
    "            print(\"No ModelNet conversions needed.\")\n",
    "        try:\n",
    "            shutil.rmtree(top_src)\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(\"ModelNet40 done.\")\n",
    "\n",
    "    # ---------- ABC Dataset ----------\n",
    "    def prepare_abc_dataset(self):\n",
    "        url = \"https://archive.nyu.edu/retrieve/120666/abc_full_100k_v00.zip\"\n",
    "        zip_path = os.path.join(self.root_dir, \"abc_full_100k_v00.zip\")\n",
    "        extracted_path = os.path.join(self.root_dir, \"100k\")\n",
    "        models_out = os.path.join(self.paths[\"abc\"], \"models\")\n",
    "        os.makedirs(models_out, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(models_out) and len(os.listdir(models_out)) > 1000:\n",
    "            print(\"ABC dataset seems processed; skipping.\")\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(extracted_path):\n",
    "            print(\"Downloading ABC dataset (large)...\")\n",
    "            self._parallel_downloads([(url, zip_path)], desc=\"Downloading ABC\")\n",
    "            if not os.path.exists(zip_path):\n",
    "                print(\"ABC zip not present after download.\")\n",
    "                return\n",
    "            print(\"Extracting ABC (this will take a while)...\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                z.extractall(self.root_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        jobs = []\n",
    "        for split in [\"train/512\", \"test/512\"]:\n",
    "            src_dir = os.path.join(extracted_path, split)\n",
    "            if not os.path.exists(src_dir):\n",
    "                continue\n",
    "            for file in os.scandir(src_dir):\n",
    "                if file.name.endswith(\".obj\"):\n",
    "                    dst = os.path.join(models_out, file.name.replace(\".obj\", \".stl\"))\n",
    "                    if os.path.exists(dst):\n",
    "                        continue\n",
    "                    jobs.append((file.path, dst, \"general\"))\n",
    "\n",
    "        if jobs:\n",
    "            print(f\"Converting {len(jobs)} ABC models...\")\n",
    "            res = self._parallel_mesh_convert(jobs, desc=\"ABC conversions\")\n",
    "            failed = sum(1 for e in res.values() if e)\n",
    "            print(f\"ABC conversions complete. Failed: {failed}\")\n",
    "        else:\n",
    "            print(\"No ABC conversions needed.\")\n",
    "\n",
    "        if os.path.exists(extracted_path):\n",
    "            try:\n",
    "                shutil.rmtree(extracted_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not remove ABC extracted folder: {e}\")\n",
    "\n",
    "    # ---------- ShapeNet (incremental processing to avoid temp bloat) ----------\n",
    "    def prepare_shapenet(self):\n",
    "        out_root = self.paths[\"shapenet\"]\n",
    "        os.makedirs(out_root, exist_ok=True)\n",
    "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "        if not hf_token:\n",
    "            print(\"HF_TOKEN not set; skipping ShapeNet.\")\n",
    "            return\n",
    "        try:\n",
    "            from huggingface_hub import login, snapshot_download\n",
    "\n",
    "            login(token=hf_token)\n",
    "            repo_id = \"ShapeNet/ShapeNetCore\"\n",
    "            cache_dir = snapshot_download(repo_id=repo_id, repo_type=\"dataset\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch ShapeNet from HF Hub: {e}\")\n",
    "            return\n",
    "\n",
    "        ctx = self._use_process_pool()\n",
    "        use_processes = ctx is not None\n",
    "\n",
    "        # iterate zips one-by-one -> process their models incrementally\n",
    "        for zip_filename in tqdm(\n",
    "            sorted(os.listdir(cache_dir)), desc=\"Enumerate ShapeNet zips\"\n",
    "        ):\n",
    "            if not zip_filename.endswith(\".zip\"):\n",
    "                continue\n",
    "            cat_id = zip_filename[:-4]\n",
    "            models_dir = os.path.join(out_root, cat_id, \"models\")\n",
    "            images_dir = os.path.join(out_root, cat_id, \"images\")\n",
    "            os.makedirs(models_dir, exist_ok=True)\n",
    "            os.makedirs(images_dir, exist_ok=True)\n",
    "            zip_filepath = os.path.join(cache_dir, zip_filename)\n",
    "\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_filepath, \"r\") as z:\n",
    "                    all_files = z.namelist()\n",
    "                    model_ids = sorted(\n",
    "                        list({f.split(\"/\")[1] for f in all_files if f.count(\"/\") >= 2})\n",
    "                    )\n",
    "\n",
    "                    # We'll process models in a controlled concurrency pattern:\n",
    "                    # submit up to max_workers_cpu conversions, wait for at least one to free up, delete tmpdir immediately.\n",
    "                    if use_processes:\n",
    "                        pool = ProcessPoolExecutor(\n",
    "                            max_workers=self.max_workers_cpu, mp_context=ctx\n",
    "                        )\n",
    "                    else:\n",
    "                        pool = ThreadPoolExecutor(\n",
    "                            max_workers=max(1, min(self.max_workers_cpu, 4))\n",
    "                        )\n",
    "\n",
    "                    futures = {}  # future -> tmp_dir for cleanup\n",
    "                    try:\n",
    "                        for mid in tqdm(\n",
    "                            model_ids, desc=f\"Models in {cat_id}\", leave=False\n",
    "                        ):\n",
    "                            # locate the candidate model .obj path inside zip (prefer model_normalized.obj)\n",
    "                            model_norm = f\"{cat_id}/{mid}/models/model_normalized.obj\"\n",
    "                            candidate = (\n",
    "                                model_norm\n",
    "                                if model_norm in all_files\n",
    "                                else next(\n",
    "                                    (\n",
    "                                        f\n",
    "                                        for f in all_files\n",
    "                                        if f.startswith(f\"{cat_id}/{mid}/models/\")\n",
    "                                        and f.endswith(\".obj\")\n",
    "                                    ),\n",
    "                                    None,\n",
    "                                )\n",
    "                            )\n",
    "                            if not candidate:\n",
    "                                continue\n",
    "                            dst = os.path.join(models_dir, f\"{mid}.stl\")\n",
    "                            if os.path.exists(dst):\n",
    "                                continue\n",
    "\n",
    "                            # extract only the model file to a fresh tmp dir\n",
    "                            tmp_dir = tempfile.mkdtemp(prefix=\"shapenet_\")\n",
    "                            extracted_ok = False\n",
    "                            try:\n",
    "                                z.extract(candidate, path=tmp_dir)\n",
    "                                # ensure we can find the file (zip may or may not preserve dirs)\n",
    "                                src_path = os.path.join(tmp_dir, candidate)\n",
    "                                if not os.path.exists(src_path):\n",
    "                                    # fallback search\n",
    "                                    found_obj = None\n",
    "                                    for root, _, files in os.walk(tmp_dir):\n",
    "                                        for f in files:\n",
    "                                            if f.endswith(\".obj\"):\n",
    "                                                found_obj = os.path.join(root, f)\n",
    "                                                break\n",
    "                                        if found_obj:\n",
    "                                            break\n",
    "                                    if found_obj:\n",
    "                                        src_path = found_obj\n",
    "                                    else:\n",
    "                                        raise RuntimeError(\n",
    "                                            \"Could not locate extracted OBJ after extract\"\n",
    "                                        )\n",
    "                                extracted_ok = True\n",
    "                            except Exception as ex:\n",
    "                                shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "                                print(\n",
    "                                    f\"Error extracting {candidate} from {zip_filename}: {ex}\"\n",
    "                                )\n",
    "                                continue\n",
    "\n",
    "                            # extract screenshots for this model from zip (only screenshots/)\n",
    "                            # they are small; copy to images_dir\n",
    "                            screenshot_prefix = f\"{cat_id}/{mid}/screenshots/\"\n",
    "                            for f in all_files:\n",
    "                                if f.startswith(screenshot_prefix) and (\n",
    "                                    f.endswith(\".png\")\n",
    "                                    or f.endswith(\".jpg\")\n",
    "                                    or f.endswith(\".jpeg\")\n",
    "                                ):\n",
    "                                    outname = f\"{mid}_{os.path.basename(f)}\"\n",
    "                                    outpath = os.path.join(images_dir, outname)\n",
    "                                    if not os.path.exists(outpath):\n",
    "                                        try:\n",
    "                                            with (\n",
    "                                                z.open(f) as src,\n",
    "                                                open(outpath, \"wb\") as dstf,\n",
    "                                            ):\n",
    "                                                shutil.copyfileobj(src, dstf)\n",
    "                                        except Exception:\n",
    "                                            pass\n",
    "\n",
    "                            # submit conversion job\n",
    "                            if use_processes:\n",
    "                                future = pool.submit(\n",
    "                                    _convert_to_stl_worker, src_path, dst\n",
    "                                )\n",
    "                            else:\n",
    "                                future = pool.submit(\n",
    "                                    _convert_to_stl_worker, src_path, dst\n",
    "                                )\n",
    "                            futures[future] = tmp_dir\n",
    "\n",
    "                            # if we've reached pool capacity, wait for at least one to complete, then cleanup that tmpdir\n",
    "                            while len(futures) >= max(1, self.max_workers_cpu):\n",
    "                                done, not_done = wait(\n",
    "                                    futures.keys(), return_when=FIRST_COMPLETED\n",
    "                                )\n",
    "                                for fut in done:\n",
    "                                    tmp = futures.pop(fut, None)\n",
    "                                    try:\n",
    "                                        dst_path, err = fut.result()\n",
    "                                        if err:\n",
    "                                            logging.debug(\n",
    "                                                f\"ShapeNet conversion error for {dst_path}: {err}\"\n",
    "                                            )\n",
    "                                    except Exception as e:\n",
    "                                        logging.debug(\n",
    "                                            f\"ShapeNet conversion raised: {e}\"\n",
    "                                        )\n",
    "                                    # cleanup tmp dir\n",
    "                                    if tmp:\n",
    "                                        try:\n",
    "                                            shutil.rmtree(tmp)\n",
    "                                        except Exception:\n",
    "                                            pass\n",
    "\n",
    "                        # after submitting all models, wait for remaining futures and cleanup\n",
    "                        for fut in as_completed(futures.keys()):\n",
    "                            tmp = futures.pop(fut, None)\n",
    "                            try:\n",
    "                                dst_path, err = fut.result()\n",
    "                                if err:\n",
    "                                    logging.debug(\n",
    "                                        f\"ShapeNet conversion error for {dst_path}: {err}\"\n",
    "                                    )\n",
    "                            except Exception as e:\n",
    "                                logging.debug(f\"ShapeNet conversion raised: {e}\")\n",
    "                            if tmp:\n",
    "                                try:\n",
    "                                    shutil.rmtree(tmp)\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "\n",
    "                    finally:\n",
    "                        pool.shutdown(wait=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing ShapeNet zip {zip_filename}: {e}\")\n",
    "\n",
    "        print(\"ShapeNet done.\")\n",
    "\n",
    "    # ---------- Custom dataset (Yandex.Disk) ----------\n",
    "    def prepare_custom_dataset(self):\n",
    "        out_root = self.paths[\"custom\"]\n",
    "        os.makedirs(out_root, exist_ok=True)\n",
    "        if any(os.scandir(out_root)):\n",
    "            print(\"Custom dataset folder not empty; skipping.\")\n",
    "            return\n",
    "\n",
    "        files_to_download = {\n",
    "            \"train_data\": \"https://disk.yandex.ru/d/RRXJu9ZtEmSXzQ\",\n",
    "            \"test_data\": \"https://disk.yandex.ru/d/TmbB7BsGzg1dQQ\",\n",
    "        }\n",
    "        tasks = []\n",
    "        for key, public_url in files_to_download.items():\n",
    "            api = \"https://cloud-api.yandex.net/v1/disk/public/resources/download?\"\n",
    "            api_url = api + urlencode(dict(public_key=public_url))\n",
    "            try:\n",
    "                r = requests.get(api_url, timeout=10)\n",
    "                r.raise_for_status()\n",
    "                href = r.json().get(\"href\")\n",
    "                if href:\n",
    "                    out_zip = os.path.join(self.root_dir, f\"yandex_{key}.zip\")\n",
    "                    tasks.append((href, out_zip))\n",
    "                else:\n",
    "                    print(f\"Could not resolve Yandex link {public_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Yandex API error for {public_url}: {e}\")\n",
    "\n",
    "        if tasks:\n",
    "            print(\"Downloading custom dataset zips from Yandex.Disk (parallel)...\")\n",
    "            dl_res = self._parallel_downloads(tasks, desc=\"Yandex downloads\")\n",
    "            for zipfile_path, err in dl_res:\n",
    "                if err:\n",
    "                    print(f\"Download failed for {zipfile_path}: {err}\")\n",
    "                    continue\n",
    "                try:\n",
    "                    with zipfile.ZipFile(zipfile_path, \"r\") as z:\n",
    "                        z.extractall(out_root)\n",
    "                    os.remove(zipfile_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract {zipfile_path}: {e}\")\n",
    "        print(\"Custom dataset done.\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# If executed as a script\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Recommended: run this script from a shell for maximum stability.\n",
    "    manager = DatasetsManager(root_dir=\"data\")\n",
    "    # manager.prepare_custom_dataset()\n",
    "    # manager.prepare_all_datasets()\n",
    "    # manager.prepare_modelnet40()\n",
    "    manager.prepare_shapenet()\n",
    "    # manager.prepare_objectnet3d()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiijcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
